\chapter{Introduction}
We will first introduce three questions in incidence geometry: the projection problem, the distance set problem, and the discrete Kakeya problem in $\R^2$. Let $P$ be a discrete subset of $\R^2$.

\begin{problem}[ (Projection)]
    Let $e\in S^1$, and $\pi_e$ be the projection onto the line $l_e$. We ask the upper bound on the number of $e$ such that $\pi_e(P)\leq\frac{n}{8}$, given that $P$ is a discrete set with $|P|=n$. 
\end{problem}
\begin{problem}[ (Distance set)]
    What is the lower bound the distance set $\Delta(P)$
    \begin{equation*}
        \Delta(P)=\{|p-p'|: p,p'\in P\}
    \end{equation*}
\end{problem}
\begin{problem}[ (Discrete Kakeya/Joints problem)]
    Given a set of $m$ lines $\mathcal{L}$, such that each line $l\in\mathcal{L}$ is $m$-rich, i.e.
    \begin{equation*}
        |P\cap l|\geq m \text{ for each } l
    \end{equation*}
    Can we put a lower bound on the size of $P$.
\end{problem}

We remind ourselves of a sharp bound regarding how the lines and points intersect. Let $I(P,\lin)=\{(p,l)\in P\times \lin: p\in l\}$ 
\begin{theorem}[Szemeredi-Trotter theorem]
    For any $P\subset\R^2$, and a finite set of lines, then we have
    \begin{equation*}
        |I(P,\lin)|\lesssim \left(|P||\lin|\right)^\frac{2}{3}+|\lin|+|P|
    \end{equation*}
\end{theorem}
We will prove a weaker result for some intuition, and gain some insight into the projection problem and the discrete Kakeya problem.
\begin{proposition}[Weaker S-T]
    \label{weakst}
    In $\R^2$, we have that
    \begin{equation}
        |I(P,\lin)|\lesssim 4\min\{|P|^\frac{1}{2}|\lin|+|P|, |\lin|^\frac{1}{2}|P|+|\lin|\}
    \end{equation}
\end{proposition}
Using Proposition~\ref{weakst}, we get the following lower bound on the discrete Kakeya problem in $\R^2$.
\begin{corollary}
    we get that for a set of $m$ lines such that each line intersects the point set $P$ at least $m$ times, we get that
    \begin{equation*}
        |P|\gtrsim m^2
    \end{equation*}
\end{corollary}
\begin{note}
    The distance set problem can be realized as intersections between points and circles, instead of points and lines.
\end{note}
We make a similar conjecture in $\R^n$, for $m^{n-1}$ lines such that each line intersects the point set $P$ at least $m$ times, then we should have
\begin{equation*}
    |P|\gtrsim m^n
\end{equation*}
This statement fails for $\R^3$. Yet we could enforce some assumption to push to a nicer result.
\begin{theorem}[G-N, Joints Problem]
    For a set of $m^2$ lines such that no more than $m$ lines lie in the same plane, and each line intersects the point set $P$ at at least $m$ points, then we have
    \begin{equation*}
        |P|\gtrsim m^3
    \end{equation*}
    (This is in fact a conjecture by Bourgain and a corollary to the Joints problem in $\R^3$).
\end{theorem}
We now prove Proposition~\ref{weakst}. \textcolor{red}{unfinished here, the key idea is to use cauchy schwartz to get an $l^2$ norm to interpret as two points. } 

We now give some general bounds on the size of $\Delta(P)$ given that $|P|=n$.
\begin{exercise}
    For a given $n\in\mathbb{N}$, there exists a set $P$ such that $|\Delta(P)|\lesssim n$, for example, the set of $n$ points arranged on a straight line.
\end{exercise}
\begin{exercise}
    We now get some general lower bound on $\Delta(P)$. We can show $|\Delta(P)|\gtrsim n^\frac{1}{2}$. Consider two distinct points $p_1, p_2$, if we show that either
    \begin{equation*}
        |\{|p_1-p|:q\in P\}|\gtrsim n^\frac{1}{2} \text{ or } |\{|p_2-q|: q\in P\}|\gtrsim n^\frac{1}{2}
    \end{equation*}
    WLOG, assume $p_1$ has that
    \begin{equation}\label{circle}
        |\{|p_1-q|: q\in P\}|\lesssim n^\frac{1}{2}
    \end{equation}
    Then we would like to show that
    \begin{equation*}
        |\{|p_2-q|: q\in P\}|\gtrsim n^\frac{1}{2}
    \end{equation*}
    If the equation~\ref{circle} is true, then there exists a distance $r$ such that 
    \begin{equation*}
        |Q|=|\{q\in P:|p_1-q|=r|\}|\gtrsim n^\frac{1}{2}
    \end{equation*}
    And for $p_1\neq p_2$, we have
    \begin{equation*}
        |\{|p_2-q|:q\in Q\}|\gtrsim n^\frac{1}{2}
    \end{equation*}
\end{exercise}

\chapter{Dimensions}
We now discuss some ways of measuring size of fractal sets.
\begin{definition}
    Given a bounded set $E$, we define its $\delta$-covering number $|E|_\delta$ as the smallest number of $\delta$-balls needed to cover $E$.
\end{definition}
We note that as $\delta\to 0$, $|E|_\delta\to\infty$, so does $\frac{1}{\delta}$, hence comparing the rate of increase between the two gives us the Minkowski dimension (box counting dimension).
\begin{example}
    Let $f: (X,d)\to (Y, d')$ is biLipschitz, if there eixts a constant $C$ such that 
    \begin{equation*}
        C^{-1}d'(f(x), f(y))\leq d(x,y)\leq Cd'(f(x), f(y))
    \end{equation*}
    Let $f:[0,1]^n\to\R^n$ be biLipschitz, where $E=f([0,1]^n)$, then we have
    \begin{equation*}
        C^{-1}E\leq |[0,1]^n|\leq CE
    \end{equation*}
    Hence $[0,1]\sim E$, and $|E|_\delta\sim\delta^{-n}$.
\end{example}

\begin{definition}[Upper and Lower Minkowski's dimension]
    Let $E$ be a bounded set in $\R^n$, and $|E|_\delta$ be the $\delta$-covering number, then we define the upper and lower Minkowski dimension as follows:
    \begin{equation*}
        \overline{\dim_B}(E)=\limsup_{\delta\to 0}\frac{\log(|E|_\delta)}{\log(1/\delta)}, \underline{\dim_B}(E)=\liminf_{\delta\to 0}\frac{\log(|E|_\delta)}{\log(1/\delta)}
    \end{equation*}

\end{definition}
\begin{example}\label{exampleQ}
    The countable set $E=\mathbb{Q}
    \cap[0,1]$, has Lebesgue measure 0, and has Minkowski dimension:
    \begin{equation*}
        \dim_B(E)=\lim_{\delta\to 0}\frac{\log(\delta^{-1})}{\log(\delta^{-1})}=1
    \end{equation*}
\end{example}
\begin{example}
    The set $E=\{\frac{1}{n}:n\in\mathbb{N}\}$ has Minkowski dimension: for every $\frac{1}{n}$, it could be covered by a $\delta=n^{-2}$-length disjoint interval, hence
    \begin{equation*}
        \dim_B(E)=\lim_{\delta\to 0}\frac{\log(n)}{\log(n^2)}=\frac{1}{2}
    \end{equation*}
\end{example}
\begin{example}
    The set $E=\{\frac{1}{2^n}: n\in\mathbb{N}\}$ is ``too sparse'' of a fractal so its box counting dimension is the same as the topological dimension.
    \begin{equation*}
        \dim_B(E)=\lim_{\delta\to 0}\frac{\log(n)}{\log(2^n)}=\lim_{n\to\infty}\frac{\log(n)}{n\log(2)}=0
    \end{equation*}
    One could generalize this to get any set $E=\{a^{-n}: n\in\mathbb{N}\}$ has Minkowski dimension 0.
\end{example}
\begin{example}
    The Cantor set, splits into $2^n$ intervals of length $\frac{1}{3^n}$.
    \begin{equation*}
        \dim_B(E)=\lim_{\delta\to0}\frac{\log(2^n)}{\log(3^n)}=\frac{\log(2)}{\log(3)}
    \end{equation*}
\end{example}
\begin{note}
    Minkowski dimension does not always exist if the upper or lower Minkowski dimensions don't agree, and it does not work with unbounded sets $E$.
\end{note}
\begin{note}
    The example~\ref{exampleQ} has Minkowski dimension 1, but it is a countable set, hence we would like to assign it measure 0.
    \begin{equation*}
        \dim\cup_iE_i=\sup_{i}\dim E_i
    \end{equation*}
\end{note}
To address the above two concerns, we introduce the Hausdorff dimension. We do it in three steps: introduce an up-to-$\delta$-cover $\{U_j\}$, construct Hausdorff $\delta$-measure, and letting $\delta\to 0$.
\subsection{Hausdorff measure}
\begin{definition}[$s$-dim Hausdorff measure]
    Fix $s\geq 0$, and $\delta\in(0,\infty]$, given a set $E\in\R^n$, an ``up-to-$\delta$''-cover of $E$ is a \textbf{countable} family of sets $\{U_j\}_{j\in\mathbb{N}}$ such that
    \begin{equation*}
        E\subset\cup_jU_j, diam(U_j)\leq\delta, \text{ for all } j
    \end{equation*}
    And an $s$-dimensional Hausdorff $\delta$-meausre of the set $E$ is
    \begin{equation*}
        H_\delta^s(E)=\inf\left\{\sum_jdiam(U_j)^s, \{U_j\}_j \text{ is an up-to-$\delta$-cover of } E \right\}
    \end{equation*}
    Finally, the $s$-dimensional Hausdorff measure of $E$ is
    \begin{equation*}
        H^s(E)=\lim_{\delta\to 0}H_\delta^s(E)
    \end{equation*}
\end{definition}
\begin{remark}
    The limit is well justified since as $\delta\to 0$, $H_\delta^s(E)$ is an increasing function.
\end{remark}
There are many nice properties regarding the Hausdorff measure, for example, $n$-dim Hausdorff measure agrees with the $n$-dim Lebesgue measure, and there is a unique number such that the Hausdorff measure stops being $\infty$, and equivalently drops to zero. Hence based on this observation, we introduce the Hausdorff dimension of a set $E$.
\begin{definition}[Hausdorff dimension]
    For a set $E\subset\R^n$, we have
    \begin{equation*}
        \dim_H(E)=\sup\{s: H^s(E)=\infty\}=\inf\{s: H^s(E)=0\}
    \end{equation*}
\end{definition}
Before anything, we first check that the $s$-dimensional Hausdorff measure defined above is indeed a measure.
\begin{proposition}
    For $s\geq 0$, the $s$-dimensional measure is indeed a measure.
\end{proposition}
\begin{proof}
    We have that $\mu(\emptyset)=0$, and $\mu(E)\geq 0$ for all $E$. Finally we check the measure is countably additive. For $\{E_j\}_{j\in\mathbb{N}}$ disjoint sets, we consider $E=\cup_jE_j$, as $\delta\to 0$, (or for $\delta$ sufficiently small, given $E_j$'s are disjoint), all the up-to-$\delta$-covers are disjoint, hence
    \begin{equation*}
        H_\delta^s(\cup_jE_j)=\sum_j H_\delta^s(E_j)
    \end{equation*}
    And letting $\delta\to 0$, we get
    \begin{equation*}
        H^s(\cup_jE_j)=\sum_jH^s(E_j)
    \end{equation*}
\end{proof}
\qed

\begin{proposition}
    The following are basic facts about the Hausdorff measure:
    \begin{enumerate}
        \item for $n\in\mathbb{N}$, let $m$ be the $n$-dim Lebesgue measure, there exists a constant $C$ such that
        \begin{equation*}
            C^{-1}H^n(E)\leq m(E)\leq CH^n(E)
        \end{equation*}
        \item $H^s(E)$ is a nonincreasing function of $s$.
        \item For $0\leq s_1<s_2<\infty$
        \begin{equation*}
            \text{either } H^{s_1}(E)=\infty \text{ or } H^{s_2}(E)=0
        \end{equation*}
        \item For $s>n$, and $E\subset\R^n$, we have that
        \begin{equation*}
            H^s(E)=0
        \end{equation*}
        \item For $E\subset\R^n$, and $s\geq 0$, we have that
        \begin{equation*}
            H^s(E)=0 \iff H_\infty^s(E)=0
        \end{equation*}
    \end{enumerate}
\end{proposition}
\begin{example}
    For a set $E\subset\R^n$, we have that the $n$-dimensional Hausdorff measure should agree with the standard Lebesgue measure on $\R^n$. For if $E$ is unbounded, then $m(E)=\infty$, and 
\end{example}


\begin{exercise}
    We have that for $f: A\to R^m, A\subset R^n$, for a fixed $s\geq 0$, and $f$ is Lipschitz with Lipschitz constant $L$, we have that 
    \begin{equation*}
        H^s(f(A))\lesssim_LH^s(A)
    \end{equation*}
\end{exercise}
\begin{proof}
    For any up-to-$\delta$ cover $\{E_j\}$ of $A$, we have $\{f(E_j)\}_j$ is an (up-to-some constant)-$\delta$ cover of $f(A)$, hence 
\end{proof}

\begin{proposition}
    The Hausdorff measure is monotone: for $E_1\subset E_2$, we have that
    \begin{equation*}
        H^s(E_1)\leq H^s(E_2)
    \end{equation*}
\end{proposition}
\begin{proof}
    For $E_1\subset E_2$, for each $\delta$,  an up-to-$\delta$-cover of $E_2$ is also an up-to-$\delta$ cover of $E_1$, and hence taking the infimimum, we get that $H^s(E_1)\leq H^s(E_2)$.
\end{proof}
\qed


\begin{proposition}
    The Hausdorff dimension satisfies that the dimension is a local property:
    \begin{equation*}
        \dim(\cup_jE_j)=\sup_j\dim(E_j)
    \end{equation*}
\end{proposition}
\begin{proof}
    We would like to show that $H^s(\cup_jE_j)=\infty$ if and only if $\sup_jH^s(E_j)=\infty$, and similarly, $H^s(\cup_jE_j)=0$ if and only if $\sup_jH^s(E_j)=0$.

    This is a total of 4 directions. By monotonicity, two directions are shown:
    \begin{equation*}
        \sup_jH^s(E_j)=\infty \Rightarrow H^s(\cup_jE_j)=\infty
    \end{equation*}

    Moreover,
    \begin{equation*}
        H^s(\cup_jE_j)=0\Rightarrow \sup_jH^s(E_j)=0
    \end{equation*}

    Moreover, by $H^s$ being a measure, if we have $\sup_jH^s(E_j)=0$, then all $H^s(E_j)=0$ for all $j$, thus
    \begin{equation*}
        H^s(\cup_jE_j)\leq\sum_jH^s(E_j)=0
    \end{equation*}
    Now it remains to show that \textcolor{red}{what}

\end{proof}


Now we justify the usage of $H^s$, instead of just working $H_\delta^s$.
\begin{exercise}
    For $0\leq s\leq 1, n\geq 2$, we have
    \begin{equation*}
        H_2^s(B_1)=H_2^s(\overline{B_1})=H_2^s(\partial(B_1))
    \end{equation*}
    We see that
    \begin{equation*}
        H_2^s(B)=H_2^s(\overline{B})=2
    \end{equation*}
    Then $H_2^s(\partial B)=0$ if $\overline{B}$ was indeed measurable. But for $0\leq s\leq $, it is more reasonable to cover $\overline{\partial B}$ with bigger covers.
\end{exercise}

Hence we work with $H^s$ to get a Borel regular measure. Recall the following definitions.
\begin{definition}
    A measure $\mu$ is a Borel measure if all Borel sets are $\mu$-measurable. Moreover, $\mu$ is called Borel regular if for any Borel set $A$, there exists another Borel set $B$ such that $B\subset A$, and $\mu(A)=\mu(B)$.
\end{definition}
With our construction, we claim that the Hausdorff measure $H^s$ for any $s>0$ is a Borel regular measure.
\begin{proposition}
    $H_\delta^s$ is a Borel regular measure.
\end{proposition}
\begin{proof}
    We first accept the fact that every Borel set is $H^s$-measurable. We show that $H^s$ is Borel-regular. For a Borel set $A$, we would like to approximate it by ``fattening up'' the covers. For each $n$, let $B_n:=\cup_jE_{n,j}$ be a cover of $A$, and such that $\sum_{j}(diam(E_{n,j}))^s\leq H_\frac{1}{n}^s(A)+\frac{1}{n}$. Then if we take $B=\cap_nB_n$, we have that $A\subset B$, and $H^s(A)=\cap_n H_\frac{1}{n}^s(A)\geq\sum_j(diam(E_{n,j}))^s-\frac{1}{n}\geq \cap_nH_\frac{1}{n}^s(B_n)-\frac{1}{n}$, which by our construction, is $H^s(B)$. Then by monotonicity of $H^s$, we have that
    \begin{equation*}
        H^s(A)=H^s(B)
    \end{equation*}
\end{proof}
\begin{note}
    The countably additivity of $H^s$ comes from the fact that all Borel sets are $H^s$-measurable, and any meausure if countably additive on its measurable sets.
\end{note}

\textcolor{red}{Section 3}
This is part to be typed up. We did Mass distribution principle, which states that if $E$ has that a $r_0$ Frostman measure $\mu$, then $H_{r_0}(E)\geq\mu(E)/C$, and if further we have that $\mu(E)>0$, then $\dim_HE\geq s$.

\begin{enumerate}
    \item Frostman implies positive Hausdorff dimension
    \item definition of support of a measure 
    \item push-forward measure 
\end{enumerate}


This is page 12 on weak convergence of measures.
\begin{definition}[Weak convergence of measures]
    Let $\{\mu_j\}$ be a sequence of locally finite measures (they automatically assign finite measures to all compact sets), and we say $\{\mu_j\}$ converges to $\mu$ weakly if for all $\varphi\in C_c(X)$, we have
    \begin{equation*}
        \lim_{j\to\infty}\int\varphi d\mu_j=\int \varphi d\mu
    \end{equation*}
\end{definition}

Our goal for tonight is to understand the proof of the Frostman Lemma.
\begin{lemma}[Frostman Lemma]
    Assume $E\subset\R^n$ is a compact set with $H^s(E)>0$, then there exists a compactly supported Borel measure $\mu$ with $supp(\mu)\subset E$ and $\mu(E)\gtrsim H_\infty^s(E)$, and such that for all $x\in\R^n, r>0$, we have
    \begin{equation*}
        \mu(B(x,r))\leq r^s
    \end{equation*}
\end{lemma}
\begin{proof}
    
\end{proof}

There are some things we need to establish before the we prove the Frostman lemma. For a set $E\in\R^n$, we use $\mathcal{M}(E)$ to denote the set of finite Borel measures whose support is contained in $E$, i.e. if $\mu\in\mathcal{M}(E)$, we have
\begin{equation*}
    supp(\mu)\subset E, \text{ and }0<\mu(E)<\infty
\end{equation*}

Next we state the ``Bolzano-Weierstrass'' theorem for measures.
\begin{lemma}
    Let $\{\mu_j\}$ be a sequence of locally finite Borel measures on $\R^n$, i.e., for all $K$ compact subset in $\R^n$, we have
    \begin{equation*}
        \sup_{j\in\mathbb{N}}\mu_j(K)<\infty
    \end{equation*}
    Then there exists a subsequence $\mu_{j_k}$ such that as $k\to\infty$, the subsequence converges to $\mu$.
\end{lemma}


\section{Hausdorff dimension of product sets}
\begin{theorem}[Hausdorff dimension of product sets]
    Let $A,B$ be Borel sets, and $s,t\geq 0$, then we have
    \begin{equation*}
        H_\infty^{s+t}(A\times B)\gtrsim_{d_1, d_2}H_\infty^s(A)H_\infty^t(B)
    \end{equation*}
\end{theorem}
\begin{proof}
    We use the theorem that positive Hausdorff meausre if and only if there exists a Frostman measure.
    
    Assume $H_\infty^s(A)>0, H_\infty^t(B)>0$, then there exists $\mu_1, \mu_2$ such that 
    \begin{equation*}
        \mu_1(A)\gtrsim H_\infty^s(A), \mu_2(B)\gtrsim H_\infty^t(B)
    \end{equation*}
    Then we consider any ball $B((x_1, x_2), r)$, we have that
    \begin{equation*}
        B((x_1, x_2), r)\subset B(x_1, r)\times B(x_2, r)
    \end{equation*}
    Hence we have
    \begin{equation*}
        \mu_1\times\mu_2(A\times B)\gtrsim r^{s+t}
    \end{equation*}
    Hence $\mu_1\times\mu_2$ is a Frostman measure on $A\times B$, hence 
    \begin{equation*}
        H_\infty^{s+t}(E)\geq r^{s+t}\gtrsim H_\infty^s(A)H_\infty^t(B)
    \end{equation*}

\end{proof}
\begin{corollary}
    For $A,B$ Borel sets, we have
    \begin{equation*}
        \dim_H(A\times B)\geq \dim_H(A)+\dim_H(B)
    \end{equation*}
\end{corollary}
\begin{proof}
    Once we have $ H_\infty^{s+t}(A\times B)\gtrsim_{d_1, d_2}H_\infty^s(A)H_\infty^t(B)$, it is easy to see, if $\dim_H(A)> s$, and $\dim_H(B)>t$, then we have that $H_\infty^s(A)\geq 0$, and $H_\infty^t(B)\geq 0$, thus $H_\infty^{s+t}(A\times B)\geq 0$, hence we have
    \begin{equation*}
        \dim_H(A\times B)\geq s+t
    \end{equation*}
\end{proof}
\qed

\begin{corollary}
    We also have that
    \begin{equation*}
        \dim_H(A\times B)\leq\dim_HA+\overline{\dim_M}B
    \end{equation*}
\end{corollary}
\begin{proof}
    If we assume that $\dim_H(A)<s, \dim_H(B)\leq\overline{\dim_M}(B)<t$, then we have
    \begin{equation*}
        \dim_H(B)<t, i.e. H_\infty^t(B)=0
    \end{equation*}
    \begin{equation*}
        \dim_\infty(A\times B)
    \end{equation*}
\end{proof}



\section{Riesz Energies}
We revisit the question regarding projections.
Assume $E$ is compact,do we always have
\begin{equation*}
    \dim_H(\pi_e(E))=\min\{\dim_H(E), 1\}
\end{equation*}
for some $e\in S^1$ or for all $e\in S^1$.

The important implication is we would like to transfer an $s$-dimensional Frostman measure on $E$ to an $s$-dimensional Frostman measure(the push-forward measure) on $\pi_e(E)$, i.e. we would like to have if $\mu(B(x,r))\lesssim r^s$ for $supp(\mu)\subset E$,
\begin{equation*}
    \mu_{\pi_e}(B(x,r))\lesssim r^s
\end{equation*}
where $supp(\mu_{\pi_e})\subset \pi_e(E)$.

\begin{definition}[Riesz potential and energy of measures]
    Let $0\leq s\leq d$, and let $\mu$ be a Borel measure on $\R^d$. The $s$-dimensional Riesz potential of the measure $\mu$ at a particular point is
    \begin{equation*}
        V_s(\mu)(x)=\mu\ast k_s(x)=\int\frac{1}{|x-y|^s}d\mu(y)
    \end{equation*}
    where $k_s$ is the $s$-dimensional Riesz kernel 
    \begin{equation*}
        k_s(x)=\frac{1}{|x|^s}
    \end{equation*}
    And the $s$-dimensional Riesz energy of the measure $\mu$ is given by integrating the potential:
    \begin{equation*}
        I_s(\mu)=\int V_s(\mu)d\mu(x)=\int\int\frac{1}{|x-y|^s}d\mu(x)d\mu(y)
    \end{equation*}
\end{definition}

The reason for introducing the Riesz energies and potentials is because they almost carry the same information as a measure being $s$-Frostman.
\begin{proposition}[Finite energy if and only if $s$-Frostman]
    Let $\mu\in M(\R^d)$ be $s$-Frostman, then for all $0\leq t<s$, we have $\|V_t(\mu)\|_\infty<\infty$, and
    \begin{equation*}
        I_t(\mu)<\infty
    \end{equation*}
    Conversely, if we have finite $s$-Riesz energy, $I_s(\mu)<\infty$, then there exists a subset $B\subset\R^d$, that we have
    \begin{equation*}
        \mu(B)>0, \mu\vert_B \text{ is $s$-Frostman}
    \end{equation*}
\end{proposition}
\begin{proof}
    \textcolor{red}{computation, should do this}
\end{proof}

As an immediate corollary, we establish a relationship between finite energy of a measure with positive Hausdorff dimension.
\begin{corollary}[Finite energy and positive Hausdorff measure]
    Let $E\subset\R^d$, then $\dim_H(E)>s\geq0$, then there exists a measure $\mu$ such that $\mu\in M(E)$, and 
    \begin{equation*}
        I_s(\mu)<\infty
    \end{equation*}
    Conversely, if there exists a $\mu\in M(E)$ such that $I_s(\mu)<\infty$, then we have
    \begin{equation*}
        \dim_H(E)\geq s
    \end{equation*}
\end{corollary}

\begin{note}
    Justifying that the measure has finite energy for $H^1$ almost every $e$ is the strategy for proving Marstrand's projection theorem.
\end{note}
Now we prove the Marstrand's projection theorem.
\begin{theorem}[Marstrand's projection theorem]
    Let $E\subset\R^2$ be Borel, then
    \begin{equation*}
        \dim_H\pi_e(E)=\min\{\dim_H(E), 1\}
    \end{equation*}
    for $H^1$ almost every $e\in S^1$.
\end{theorem}


\section{Rectifiable sets}
There is actually a second part of the statement of Marstrand's projection theorem. 
\begin{theorem}
    If $\dim_H(E)>1$, then for $H^1$ almost every $e$, we have
    \begin{equation*}
        H^1(\pi_e(E))>0
    \end{equation*}
\end{theorem}
To prove this, we will use tools in section 6. However, we can ask the following question now:
\begin{problem}
    If $E\subset\R^2$ is compact, can we always have
    \begin{equation*}
        H^1(\pi_e(E))>0?
    \end{equation*}
\end{problem}
\begin{note}
We will answer this with a positive answer if $E$ is 1-rectifiable, and $0<H^1(E)<\infty$. And we will answer this with a negative answer, as our last theorem of the section, the Besicovitch theorem.
\end{note}
\begin{theorem}
    Let $E\subset\R^2$ be a purely 1-unrectifiable set with $0<H^1(E)<\infty$, then for $H^1$ almost every $e\in S^1$, we have
    \begin{equation*}
        H^1(\pi_e(E))=0
    \end{equation*}
\end{theorem}


We define the densities at a particular point.
\begin{definition}[$s$-dimensional density]
    Let $A\subset\R^d$, and $H^s(A)<\infty$, for $s\geq 0$.
    The upper and lower $s$-dimensional densities of $A$ at $x$ is defined to be
    \begin{equation*}
        \Theta^{s,*}(A,x)=\limsup_{r\to 0}\frac{H^s(A\cap B(x,r))}{r^s}, \Theta_*^s(A,x)=\liminf_{r\to 0}\frac{H^s(A\cap B(x,r))}{r^s}
    \end{equation*}
\end{definition}

\begin{note}
    The following should be interpreted just as the Lebesgue point density theorem.
\end{note}
The following two propositions are just like Lebesgue density theorems, and I shall call them the analogs of Hausdorff  density theorems. This implies that depending on whether $x$ is in the set or not, the density at that point is either positive or 0.
\begin{proposition}[Hausdorff density theorem]
    For $H^s$ almost all $x\in A$.
    \begin{equation*}
        1\leq\Theta^{s,*}(A,x)\leq 2^s
    \end{equation*}
\end{proposition}
And it is what we expected for $x\not\in A$.
\begin{proposition}
    For $H^s$ almost all $x\in\R^d\setminus A$, we have
    \begin{equation*}
        \Theta^{s,*}(A,x)=0
    \end{equation*}
\end{proposition}

\subsection{Rectifiable sets}
Let's define the two types of sets that answer oppositely to our positive projection theorem.
\begin{definition}[rectifiable]
    Let $0<n<d$ be integers, and let $E\subset\R^d$, and we say that $E$ is $n$-rectifiable if for $H^n$ almost all of $E$ can be covered by Lipschitz images of $\R^n4$. In other words, there exists a set of countable Lipschitz maps $\{f_j\}, f_j:\R^n\to\R^d$ such that
    \begin{equation*}
        H^n\left( E\setminus \bigcup_jf_j(\R^n)\right)=0
    \end{equation*}
\end{definition}
\begin{definition}[n-unrectifiable]
    $E$ is called purely $n$-unrectifiable if for all Lipschitz map $f:\R^n\to\R^d$, we have
    \begin{equation*}
        H^n(E\cap f(\R^n))=0
    \end{equation*}
    (This means that $E$ does not intersect nontrivially with any $f(\R^n)$). 

    Alternatively, $E$ is $n$-unrectifiable if and only if for all $n$-rectifiable sets $R\subset\R^d$, we have
    \begin{equation*}
        H(E\cap R)=0
    \end{equation*}
\end{definition}
The following asserts that any $E\subset\R^d$ with $H^n(E)<\infty$ can be decomposed into $n$-rectifiable sets, and purely $n$-unrectifiable sets.
\begin{theorem}[Rectifiable decompositions]
    Let $0<n<d$, and let $E\subset\R^d$, and $H^n(E)<\infty$, and then there exists $R\subset\R^d$, $n$-rectifiable, such that $E$ can be written into disjoint unions of rectifiable and unrectifiable portions:
    \begin{equation*}
        E=[E\cap R]\cup U
    \end{equation*}
    where $U=E\setminus R$ is purely $n$-unrectifiable, and $E\cap R$ is $n$-rectifiable.
\end{theorem}
\begin{note}
    This decomposition is unique up to null sets.
\end{note}
\begin{proof}
    \textcolor{red}{finish this}
\end{proof}

\subsection{Projections of rectifiable sets}
\begin{theorem}
    For $E\subset\R^2$, if $E$ is 1-rectifiable with $0<H^1(E)<\infty$, and if $e_1, e_2\in S^1$ are distinct vectors such that 
    \begin{equation*}
        H^1(\pi_{e_1}(E))=H^1(\pi_{e_2}(E))=0
    \end{equation*}
    then $e_1=-e_2$.
\end{theorem}
\begin{proof}
    We show that there exists a direction $e_0$ such that for all $e\in S^1\setminus e_0^\perp$, we have
    \begin{equation*}
        H^1(\pi_e(E))>0
    \end{equation*}
    Then this would imply that if two directions $e_1, e_2$ have zero length projections, then they both must live in $e_0^\perp$, hence $e_1=-e_2$.

    For $E$ 1-rectifiable, there exists an open interval $J$ and some Lipschitz function $f$ such that 
    \begin{equation*}
        H^1(E\cap f(J))>0
    \end{equation*}
    For any Lipschitz function, we can approximate it well with a $C^1$ function $g$ such that 
    \begin{equation*}
        H^1(\{x: f(x)\neq g(x)\})=0
    \end{equation*}
    And via computation, we also have
    \begin{equation*}
        H^1(E\cap g(J))>0
    \end{equation*}
    Then let
    \begin{equation*}
        F_1=\{t\in J: g'(t)=0\}, F_2=\{t: \liminf_{r\to 0}\frac{H^1((E\cap B(t,r)))}{r}=0\}
    \end{equation*}
    We would like to show that $H^1(g(F_1))=H^2(E\cap g(F_2))=0$, and by $H^1(E\cap g(J))>0$, we know there must exist some point $t_0\in J$ such that 
    \begin{equation*}
        g'(t_0)\neq 0, \text{ and }\liminf_{r\to 0}\frac{H^1(E\cap B(t,r))}{r}>0
    \end{equation*}
    Let $e_0=\frac{g'(t_0)}{|g'(t_0)|}$, and for 
    \begin{equation*}
        e\in S^1\setminus e_0^\perp
    \end{equation*}
    We have $\pi_e$ as a biLipschitz function. And by the density at $t$, we have
    \begin{equation*}
        H^1(\pi_e(E))\geq H^1(\pi_e(E\cap B(t,r)))\gtrsim H^1(E\cap B(t,r))\gtrsim r>0
    \end{equation*}
\end{proof}
\qed




\subsection{Projection of unrectifiable sets}
We first say a bit about the ``conical'' density of unrectifiable sets. We know from the density theorem, for $H^s$ almost every $x\in E$, we have the upper $s$ dimensional density at $x$:
\begin{equation*}
    \Theta^{s,*}(E,x)=\limsup_{r\to 0}\frac{H^s(E\cap B(x,r))}{r^s}\geq 1
\end{equation*}
This is because our $B(x,r)$ is quite large. Now we ask the question, can we replace $B(x,r)$ with smaller sets, such that when we intersect it with $E$, the upper $s$-dimensional density is still postive?
\begin{definition}[Cone]
    For $S\subset S^1$, for example, an arc $J$, we define the ``cone:''
    \begin{equation*}
        C(x,S)=\bigcup_{e\in S}l_e(x)
    \end{equation*}
    where $l_e(x)$ is the line parallel to $e$ and passes through $x$.
\end{definition}
\begin{problem}
    Will the following still hold? 
    \begin{equation*}
        \Theta_J^{1,*}=\limsup_{r\to 0}\frac{H^s(E\cap B(x,r)\cap C(x,J))}{r^s}>0
    \end{equation*}
\end{problem}
\begin{note}
    We defined a new density at $x$ with the $J$ arc or cone cutoff.
\end{note}
The answer is generally no. We will demonstrate via an example
\begin{example}
    Let $E=span((1,0))$, i.e. the horizontal line in $\R^2$, and take $J=S^1\setminus (1,0)\cup (-1, 0)$. Then we see that 
    \begin{equation*}
        E\cap B(x,r)\cap C(x,J)=\{x\}
    \end{equation*}
    where the horizontal line intersects every nonhorizontal line exactly at one point $\{x\}$.
\end{example}
However, for unrectifiable sets, the above situations cannot happen. In other words, unrectifable sets are still in a sense that, for any arc we take, the $H^1$ measure of their intersections are pretty large.
\begin{theorem}[Density of unrectifiable sets]
    Let $E\subset\R^2$ be a purely 1-unrectifiable set, with $0<H^1(E)<\infty$, then for $H^1$ almost every $x\in E$, and for all arcs $J\subset S^1$, we have
    \begin{equation*}
        \Theta_J^{1,*}(E, x)\geq cH^1(J)
    \end{equation*}
    for some absolute constant $c$ independent of $J$.
\end{theorem}
To prove this theorem, we first exhibit the following lemma:
\begin{lemma}
    Let $E\subset\R^2$, and $J$ is any arc in $S^1$, if 
    \begin{equation*}
        E\cap C(x,J)=\{x\}
    \end{equation*}
    then $E$ is rectifiable.
\end{lemma}
\begin{example}
    Just like the previous example, where $E$ is just the horizontal line in $\R^2$, which is 1-rectifiable.
\end{example}
\begin{proof}[of the lemma] 
    It suffices to show that  \textcolor{red}{lemma}
    You show that $E$ can be written as the image of a Lipschitz map from $\R\to \R^2$, and this Lipschitz map is the inverse an a projection. We use the assumption carefully to produce a direction $e\in S^1$ such that is not contained in $J$ or $-J$. 
\end{proof}

Now we are ready to prove the theorem. This is to say, the unrectifiable sets carry some sense of ``density'' with them, such that when they intersect any cone, the intersection is not small.
\begin{proof}[of the theorem]
    We first note that it suffices to show this for a fixed arc, since any arc has arbitrary length $>0$. And assume $J=B((0,1), \rho)\cap S^1$, the ``north cap.'' If we let the \textbf{low density set} be denoted as $B_{\delta, \tau}$, we have
    \begin{equation*}
        B_{\delta, \tau}=\{x\in E: \mu(B(x,r)\cap C(x,J))<\tau r, 0<r<\delta\}
    \end{equation*}
    Then clearly, we have
    \begin{equation*}
        \{x\in E: \Theta_{J}^{1,*}(E,x)<\tau\}\subset\bigcup_{\delta>0}B_{\delta, \tau}
    \end{equation*}
    Hence we see that it suffices to show these sets have measure 0, i.e.
    \begin{equation*}
        H^1(B_{\delta, \tau})=0
    \end{equation*}
    However, to show this, we take an indirect proof. Fix $\delta>0$, for sufficiently large $C\geq 1$, for $x_0\in B(x_0, r_0/4)$, 
    \begin{equation*}
        H^1(B_{\delta,\tau}\cap B(x_0, r_0/4))\lesssim\frac{r_0}{C}, 0<r_0\leq\delta
    \end{equation*}
    And we then have 
    \begin{equation*}
        \Theta^{1,*}(B_{\delta,\tau}, x_0)<1
    \end{equation*}
    for all $x_0\in B_{\delta, \tau}$, since we have the Hausdorff density theorem, which states points inside a set should have density $\geq 1$, we thus have
    \begin{equation*}
        H^1(B_{\delta,\tau})=0
    \end{equation*}
    Hence it suffices to show for any compact $K\subset B_{\delta,\tau}\cap B(x_0, r_0/4)$, we have
    \begin{equation*}
        H^1(K)\lesssim \frac{r_0}{C}
    \end{equation*}
    \begin{note}
        Here is where we use the ``low-density'' property of $B_{\delta, \tau}$, where if we take $\tau=\frac{H^1(J)}{C}$, we have
        \begin{equation*}
            H^1(B(x,r)\cap C(x,J))<\tau r=\frac{H^1(J)r}{C}
        \end{equation*} 
    \end{note}
    However, this condition is bit hard to use since it is the intersection of $B(x,r)$ with a cone, and we turn this information from cones to tubes using the following lemma.
    
    \begin{lemma}[From cones to tubes]
        For $H^1$ almost every $x\in K$, there exists a vertical tube of width $0<w(T_x)\leq r_0$ around the line $l_{(0,1)}(x)$ such Theta
        \begin{equation*}
            H^1(K\cap T_x)\lesssim \frac{w(T_x)}{C}
        \end{equation*}
    \end{lemma}
    \begin{note}
        We will prove this lemma shortly after.
    \end{note}
    (By removing a null set), note that $K$ can be covered using a countable set of tubes $\{T_j\}$ such that $T_j/5$ are disjoint, and
    \begin{equation*}
        K\subset\bigcup_j T_j
    \end{equation*}
    And we thus have
    \begin{equation*}
        H^1(K)\leq \sum_j H^1(K\cap T_j)\lesssim\sum_j\frac{w(T_j)}{C}=\frac{5}{C}\sum_jw(T_j/5)\lesssim \frac{r_0}{C}
    \end{equation*}
\end{proof}
\qed

\begin{lemma}[From cones to tubes]
        For $H^1$ almost every $x\in K$, there exists a vertical tube of width $0<w(T_x)\leq r_0$ around the line $l_{(0,1)}(x)$ such Theta
        \begin{equation*}
            H^1(K\cap T_x)\lesssim \frac{w(T_x)}{C}
        \end{equation*}
\end{lemma}
\begin{proof}
    We will now prove this lemma. The key idea is that we would like to cover $T_x\cap K$ with a ball $B(, h(x))$, and also two cones.
    \begin{enumerate}
        \item $T_x\cap K\subset B(x,h(x))$
        \item We have $T_x\subset C(x,J)\cup C(x^*, J)$. And using how we will construct $B(x, h(x))$, we have $B(x^*, 2h(x))$.
    \end{enumerate}
    If we show both of the above claims, then we have that 
    \begin{equation*}
        H^1(K\cap T_x)\leq H^1(B(x,h(x))\cap C(x,J))+H^1(B(x^*, 2h(x))\cup C(x^*, J))
    \end{equation*}
    Thus by our conical estimate, we have that
    \begin{equation*}
        H^1(K\cap T_x)\lesssim \frac{h(x)H^1(J)}{C}
    \end{equation*}
    \textcolor{red}{we take a smaller region} 

    We construct a ball $B(x,h(x))$ such that $T_x\cap K\subset B(x, h(x))$.
\end{proof}







We now talk about projections of unrectifable sets. 
\begin{theorem}[Besicovitch]
    Assume $E\subset \R^2$ is Borel purely 1-unrectifiable 1-set. Then we have 
    \begin{equation*}
        H^1(\pi_e(E))=0
    \end{equation*}
    for $H^1$ almost every $e\in S^1$.
\end{theorem}
We first define the notion of high multiplicity and high density for directions $e\in S^1$.

\begin{definition}[High Multiplicity]
    We say a direction $e$ is a direction of high multiplicity at $x$, denoted $e\in H_x$, if
    \begin{equation*}
        |E\cap l_e(x)\cap B(x,r)|\geq 2, \text{ for all } r>0
    \end{equation*}
    Alternatively, for a fix $r>0$, we can denote the above as $H_x(r)$, then we have
    \begin{equation*}
        H_x=\bigcap_{r>0}H_x(r)
    \end{equation*}
\end{definition}
The condition ``for all $r>0$'' implies that there are points in $E$ that are in $l_e(x)$ that are infinitely close to $x$.

\begin{definition}[High density]
    We say $e$ is a direction of high density at $x$, denoted that $e\in D_x$, if for all $r_0, M, \epsilon>0$, there exists a radius $0<r<r_0$, and an arc $J\subset S^1$, and $e\in J$, $0<\sigma(J)<\epsilon$, we have
    \begin{equation*}
        \frac{\mu(C(x,J)\cap B(x,r))}{r}\geq M\sigma(J)
    \end{equation*}
    where $\mu=\frac{1}{|H^s(E)|}H^1\vert_E$, and $C(x, J)$ is the cone $C(x,J)=\bigcup_{e\in S}l_e(x)$.

    Alternatively, for fixed $r_0, M, \epsilon>0$, we have
    \begin{equation*}
        D_x=\bigcup_{r_0, \epsilon, M>0}D_x(r_0,\epsilon, M)
    \end{equation*}
\end{definition}
\begin{note}
    Both high multiplicities and high denstiies at $x$ mean that there are plenty of points in $E$ that are arbitrarily close to $x$, contained in an arbitrarily narrow cones around $l_e(x)$. This could give intuition that $E$ has very small projections.
\end{note}

\begin{note}
    \textbf{Proof Sketch} We will show that for $H^1_E$ almost every $x\in E$, and for almost all  $H^1_S$ directions $e$, we have that $e$ is either high multiplicity or high density. Then we will show that points with almost all directons that are either of high density or high multiplicity will have small projections in almost all directions.
\end{note}

\begin{lemma}[High density or high multiplicity]
    For $H^1$ almost every $x\in E$, and $H^1$ almost every $e\in S^1$ is either a direction of high multiplicity ($e\in H_x$) or high density at $x$ ($e\in D_x$).
\end{lemma}
\begin{proof}
    We will specify what those points are and show that $H^1(\text{not these points})$ are of measure 0. In other words, for any fixed $r_0, \epsilon, M>0$, we show that for $\mu$ almost every $x\in E$, we have
    \begin{equation*}
        \sigma(S^1\setminus[H_x(r_0)\cup D_x(r_0, \epsilon, M)])=0
    \end{equation*}
    Since $\mu$ almost every $x\in E$, satisfies 
    \begin{equation*}
        \frac{\mu(B(x,r)\cap C(x,J))}{r}\geq c\sigma(J)
    \end{equation*}
    for arbitrary arc $J\subset S^1$, it suffices to show that for almost every $e\in S^1$, we have either
    \begin{equation*}
        \Theta^{1,*}(H_x(r_0), e)>0 \text{ or  } \Theta_x^1(D_x(r_0, \epsilon, M))>0
    \end{equation*}
    And note that for $\sigma$ almost  everywhere $e\in S^1\setminus [H_x(r_0)\cup D_x(r_0, \epsilon, M)]$, we have
    \begin{equation*}
        \Theta^{1}(H_x(r_0), e)=\Theta^1(D_x(r_0, \epsilon, M), e)=0
    \end{equation*}
    Then we have 
\end{proof}



We will now show how this lemma immplies the Besicovitch projection theorem.

\textcolor{red}{unfinished here}


\chapter{Fourier transform of measures}
Recall we let $M(E)$ denote the set of finite Borel measures $\mu$ with compact support satisfying 
\begin{equation*}
    supp(\mu)\subset E, \mu(E)>0
\end{equation*}
We define the Fourier trasnform of measures as follows:
\begin{definition}[Fourier transform of measures]
    Let $\mu\in M(\R^d)$, we have 
    \begin{equation*}
        \widehat{\mu}(\xi)=\int_{\R^d}e^{-ix\cdot\xi}d\mu(x)
    \end{equation*}
    Notice the exponent that we use here is slightly different from the usual $2\pi$, but most is just notation.
\end{definition}
We then state and prove some important properties that we will use later about Fourier transform of measures.
\begin{note}
    Orponen used $\mathcal{L}^d$ to denote the $d$-dimensional Lebesgue measure, here we will simply use $dx$, whenever we are integrating with respect to the Lebesgue measure in $\R^d$.
\end{note}
\begin{proposition}
    Let $\mu\in M(\R^d)$, and let $f\in\mathcal{S}$. Then we have 
    \begin{equation*}
        \int fd\mu=\frac{1}{(2\pi)^d}\int\hat{f}\overline{\hat{\mu}}dx
    \end{equation*}
    Moreover, we have 
    \begin{equation*}
        \|f\|_{L^2}=\frac{1}{(2\pi)^d}\|f\|_{L^2}
    \end{equation*}
\end{proposition}
\begin{proof}
    The same proof that we did for duality and Plancherel.
\end{proof}
There are some useful identites that you get from this duality relation.
\begin{corollary}
    \begin{equation*}
        \int|\hat{\mu}|^2d\sigma(\xi)=\int \hat{\sigma}\ast \mu d\mu
    \end{equation*}
\end{corollary}
Later we will define the quantity on the left.

Next we give a list of properties.
\begin{lemma}
    We have 
    \begin{enumerate}
        \item Fourier transform turns convolution into pointwise multiplication:
        \begin{equation*}
            \widehat{\mu\ast f}(\xi)=\hat{\mu}(\xi)\hat{f}(\xi)
        \end{equation*}
        \item If $\hat{\mu}\in L^2$, then $\mu\in L^2$, and $\mu\ll m^d$, where $m^d$ is the Lebesgue measure.
        \item For an invertible linear transformation $T:\R^d\to\R^d$, we have 
        \begin{equation*}
            \widehat{f\circ T}(\xi)=\frac{1}{|\det(T)|}\hat{f}((T^{-1})^t\xi)
        \end{equation*}
    \end{enumerate}
\end{lemma}
\begin{proof}
    For 2, we note that it is absolutely continuous with respect to the Lebesgue measure, because by the previous proposition,
    \begin{equation*}
        f\mapsto \int fd\mu
    \end{equation*}
    is a positive linear functional defined on functions dense in $L^2$, hence by Riesz representation theorem, there exists a unique $g\in L^2$ such that 
    \begin{equation*}
        \varphi(f):=\int fd\mu=\int fgdx
    \end{equation*}
    And clearly we have $d\mu=gdx$.

    For 4, note that we do a change of variable and use the fact that for an invertible linear transformation $A$, we have 
    \begin{equation*}
        Ax\cdot y=x\cdot A^ty
    \end{equation*}
\end{proof}
\qed


Next we connect the Riesz energy of a measure with the Fourier transform of this measure (specfically, its $L^2$ behavior.) Recall that the Riesz energy is defined as 
\begin{equation*}
    I_s(\mu)=\int |x-y|^{-s}d\mu(x)d\mu(y)=\int k_s\ast \mu d\mu(x)
\end{equation*}
where $k_s$ is the $s$-dimensional Riesz kernel $|x|^{-s}$, and we define convolution with respect to a measure as 
\begin{equation*}
    k_s\ast\mu(x)=\int |x-y|^{-s}d\mu(y)
\end{equation*}
\begin{proposition}
    Let $\mu\in M(\R^d)$, let $0<s<d$, there exists some (ignorable constant ) $c$ such that 
    \begin{equation*}
        I_s(\mu)=c\int|\hat{\mu}(\xi)|^2|\xi|^{s-d}d\xi
    \end{equation*}
\end{proposition}
\begin{proof}
    This follows simply from computation (assuming there is no issue with convergence).
    \begin{align*}
        I_s(\mu)&=\int k_s\ast \mu d\mu\\
        &=\int \widehat{k_s\ast\mu}\overline{\hat{\mu}}dx\\
        &=\int \hat{k_s}\hat{\mu}\overline{\hat{\mu}}dx\\
        &=c\int |\hat{\mu}|^2|\xi|^{s-d}d\xi
    \end{align*}
\end{proof}
\qed

We will use this identity a lot. Namely, a common technique we will use is that we will attempt to define a measure $\mu$ on a set (for example, the projection measure, or the pushforward distance measure on $\Delta(E)$), and it suffices to show $E$ has positive Lebesgue measure if $\hat{\mu}\in L^2$.
\begin{note}
    One can sometimes extract that $\mu\in L^2$ given finite energy.
\end{note}

\begin{corollary}
    If $\mu\in M(E)$, and $\hat{\mu}\in L^2$, then 
    \begin{equation*}
        d\mu=fdx
    \end{equation*}
    for some $f\in L^2$, hence $|E|>0$.
\end{corollary}
We will look at one specific application of the above method.
\begin{theorem}[Marstrand's Projection theorem, II]
    Let $E\subset\R^2$ be compact. If $\dim_H(E)>1$, then for $H^1$ almost every $e\in S^1$, we have 
    \begin{equation*}
        |\pi_e(E)|>0
    \end{equation*}
\end{theorem}
\begin{proof}
    Since $\dim_H(E)>1$, there exists a measure $\mu\in M(E)$ such that $I_1(\mu)<\infty$. And our goal is to show the pushforward measure $\widehat{\pi_e\mu}\in L^2$. This is given by 
    \begin{align*}
        \int_{\R^2}|\widehat{\pi_e\mu}(\xi)|^2d\xi&=\int_{S^1}\int_{\R}|\widehat{\pi_e\mu}|^2d\xi d\sigma(\omega)\\
        &=\int_{S^1}\int_{\R}|\hat{\mu}(\xi\omega)|^2d\xi d\sigma(\omega)\\
        &=\int_{\R^2}\frac{|\mu(\xi)|^2}{|\xi|}d\xi\\
        &\sim I_1(\mu)<\infty
    \end{align*}
    Note the last $\sim$ means it differs by a constant.
\end{proof}
\qed

From the above, we see that one should expect some decay of $|\hat{\mu}(\xi)|$ from the finiteness of $I_s(\mu)$, however, this decay is in an average sense. We next define the spherical average.
\begin{definition}[Spherical average]
    Let $\sigma_r$ be the spherical measure on the ball $B(0,r)$. The spherical average of $\hat{\mu}$ is 
    \begin{equation*}
        \sigma(\mu)(r)=\frac{1}{r^{d-1}}\int_{S(0,r)} |\hat{\mu}|^2d\sigma_r
    \end{equation*}
    where $S(0,r)$ is the sphere in $\R^d$.
\end{definition}
\textcolor{red}{think about this} To agree with Wolff's notations, one can show
\begin{proposition}
    \begin{equation*}
        \sigma(\mu)(r)=\int_{S^{d-1}}\hat{\mu}(Re^{i\theta})d\theta
    \end{equation*}
    Moreover, the energy $I_s(\mu)$ is given by 
    \begin{equation*}
        I_s(\mu)=\int_{\R^n}|\hat{\mu}(\xi)|^2|\xi|^{s-d}d\xi=\int \sigma(\mu)(r)r^{s-d}dr
    \end{equation*}
\end{proposition}
How do we connect the spherical average with the positive Lebesgue measure of the distance set $|\Delta(E)|$.
\begin{proposition}[Spherical average and $|\Delta(E)|$]
    Let $\nu$ be the pushforward distance measure on $\Delta(E)$, then $\hat{\nu}\in L^2$ if and only if 
    \begin{equation*}
        \int r^{d-1}(\sigma(\mu)(r))^2 dr<\infty
    \end{equation*}
\end{proposition}
Now we discuss the case in $d=2$. 
\begin{corollary}
    If $d=2$ and $s>1$, and for some $\mu\in\Delta(E)$, we have $I_s(\mu)<\infty$, and if $\sigma(\mu)(r)\lesssim r^{s-2}$, then a subset $E\subset\R^2$, with $\dim_H(E)>s$, we have $|\Delta(E)|>0$.
\end{corollary}
\begin{proof}
    This is given by the finiteness of $I_s(\mu)<\infty$. We have 
    \begin{align*}
        \int r(\sigma(\mu)(r))^2dr&\lesssim \int r^{s-1}\sigma(\mu)(r)dr\\
        &\sim I_s(\mu)<\infty
    \end{align*}
\end{proof}
\qed

\begin{remark}
    The $s-2$ is the exponent that one needs to turn the critical integral to the finite energy. So if we can bound the spherical average with some exponent less that $s-2$, then we obtain some gain.
\end{remark}

Now if we gain new average on $\sigma(\mu)(r)$, we gain better lower bound.
\begin{theorem}[Mattila]
    For $s\geq 1$, we have 
    \begin{equation*}
        \sigma(\mu)(r)\lesssim r^{1-s}
    \end{equation*}
\end{theorem}
This is the bound that was obtained by Mattila on spherical averages.
\begin{corollary}
    Let $E\subset\R^2$, and $\mu\in M(E)$ such that $I_s(\mu)<\infty$, for all $s>3/2$, we have 
    \begin{equation*}
        |\Delta(E)|>0
    \end{equation*}
\end{corollary}
\begin{proof}
    Again, we show that 
    \begin{equation*}
        \int r(\sigma(\mu)(r))^2dr<\infty
    \end{equation*}
    This is given by 
    \begin{align*}
        \int r(\sigma(\mu)(r))^2dr&\lesssim \int r\cdot r^{1-s}\sigma(\mu)(r)dr\\
        &\lesssim r\cdot r^{s-2}\sigma(\mu)(r)dr\\
        &\sim I_s(\mu)<\infty
    \end{align*}
    The second line follows from the fact that $s>3/2$.
\end{proof}
\qed

\begin{theorem}[Wolff]
    We have 
    \begin{equation*}
        \sigma(\mu)(r)\lesssim r^{-s/2+\epsilon}
    \end{equation*}
\end{theorem}
Then exactly by the same process as above, if we want to have 
\begin{equation*}
    r^{-s/2}\lesssim r^{s-2}
\end{equation*}
We have $s>4/3$. 
\begin{corollary}
    For $E\subset\R^2$, if $\dim_H(E)>4/3$, then $|\Delta(E)|>0$.
\end{corollary}

Now we discuss Fourier dimension.
\begin{definition}[Fourier dimension]
    Let $E\subset\R^d$, the Fourier dimension of $E$ is 
    \begin{equation*}
        \dim_F(E)=\sup\{s\in [0,d]: \text{ there exists }\mu\in M(E): |\hat{\mu}(\xi)|\lesssim |\xi|^{-s/2}, \forall \xi\in\R^n\setminus\{0\}\}
    \end{equation*}
\end{definition}
We make sense of the Fourier dimension with some examples, to show the Fourier dimension needless to agree with the Hausdorff dimension in any sense.
\begin{example}
    For $S^{d-1}$, we note that the spherical measure has the good decay, in other words, 
    \begin{equation*}
        |\hat{\sigma}(\xi)|\lesssim |\xi|^{(d-1)/2}
    \end{equation*}
    Hence $\dim_F(S^{d-1})=d-1$. We see that the Fourier dimension agrees with the Lebesgue and Hausdorff dimension.
\end{example}
\begin{example}
    If we consider the set $E=[0,1]\times\{0\}$, then the Lebesgue or Hausdorff dimension is 1, however, $\dim_F(E)=0$.
\end{example}
We first relate the Fourier dimension with the Hausdorff dimension.
\begin{proposition}
    For $E\subset\R^d$, we have
    \begin{equation*}
        \dim_FE\leq \dim_HE
    \end{equation*}
\end{proposition}
\begin{proof}
    If suffices to show that if $\dim_FE>s$, then $\dim_H(E)\geq s$ for all $s>0$. One could choose $s+\epsilon$ such that there exists a $\mu$ such that 
    \begin{equation*}
        |\hat{\mu}(\xi)|\lesssim |\xi|^{-(s+\epsilon)/2}
    \end{equation*}
    To show $\dim_HE\geq s$, it suffices to show for this $\mu$, $I_s(\mu)<\infty$.
    \begin{equation*}
        I_s(\mu)=\int |\hat{\mu}(\xi)|^2|\xi|^{s-d}d\xi\lesssim \int |\xi|^{-d-\epsilon}d\xi<\infty
    \end{equation*}
\end{proof}
\qed

\begin{note}
    This is no longer an $L^2$ problem. Now we cut it into good and bad parts. But its $L^1$ norm is small.
\end{note}

\chapter{Kakeya set}
We will prove the following two results in the plane: Kakeya set in $\R^2$ has lower Minkowski dimension 2, and has Hausdorff dimension 2. 
\begin{theorem}
    Let $E$ be a Kakeya set in $\R^2$, then it has lower Minkowski dimension 2.
\end{theorem}
\begin{proof}
    We will prove the key lemma.
    \begin{lemma}
        Let $S=\{e_1, ..., e_n\}\subset S^1$ be a set of $\delta$-separated directions, and let $T_j$ be the $\delta\times 1$ tube containing a line parallel to $e_j$ (in other words, they are $\delta$-separated tubes), if we write $T=\bigcup_{j=1}^NT_j$, then we have 
        \begin{equation*}
            |T|\gtrsim \frac{N\delta}{\log(1/\delta)}
        \end{equation*}
        where $|T|=\mathcal{L}^2(T)$. This states that the $\delta$-separated tubes cannot too much intersection in volume.
    \end{lemma}
    We first show how the lemma implies the lower Minkowski dimension. We simply pick the $S$ to be the maximal set, i.e. $N\sim\delta^{-1}$. Fix any cover of $E$ using balls $B(x_1, \delta/2), ..., B(x_M, \delta/2)$ (this aligns with the definition of covering using balls of diameter $\delta$), we know that 
    \begin{equation*}
        T=\bigcup_{j=1}^N\subset \bigcup_{j=1}^M B(x_j,10\delta)
    \end{equation*}
    And since $|T|\gtrsim\frac{1}{\log(1/\delta)}$, we have 
    \begin{equation*}
        \left|\bigcup_{j=1}^M B(x_j, 10\delta)\right|\gtrsim \delta^{-2}M\gtrsim |T|
    \end{equation*}
    This gives $M\gtrsim \frac{1}{\delta^{2}\log(1/\delta)}$. Hence by definition, we have 
    \begin{equation*}
        \liminf_{\delta\to 0}\frac{\log|E|_\delta}{\log(1/\delta)}\gtrsim \liminf_{\delta\to 0}\frac{\log(\delta^{-2})+\log(\log(1/\delta))}{\log(1/\delta)}=2
    \end{equation*}
\end{proof}
\qed

\textcolor{red}{prove the lemma}
The lemma is proved by some pictures and geometric observations. If we let 
\begin{equation*}
    f=\sum_{j=1}^N\chi_{T_j}
\end{equation*}
Then it is reduced to a problem of bounding $\|f\|_{L^2}$. 
\begin{equation*}
    |T|\geq\frac{\|f\|_{L^1}^2}{\|f\|_{L^2}^2}
\end{equation*}
This is given by Cauchy Schwartz.
\begin{equation*}
    \|f\|_{L^1}^2\leq\|f\|_{L^2}^2|supp(f)|=\|f\|_{L^2}^2|T|
\end{equation*}
And bounding the $L^2$ norm of $f$ is naturally associated with intersections of two tubes.
\begin{equation*}
    \|f\|_{L^2}^2=\int \sum_i\sum_j\chi_{T_i}\chi_{T_j}dx=\sum_i\sum_j|T_i\cap T_j|
\end{equation*}

\begin{theorem}[Kakeya conjecture in $\R^2$]
    Let $E$ be a Kakeya set in $\R^2$, then it has Hausdorff dimension 2.
\end{theorem}
\begin{proof}
    We will use a pigenholing principle. 
    \begin{lemma}[pigenhole lemma]
        Let $\nu$ be a finite measure on $X$, and let $\{E_i\}_{i=1}^\infty$ be a set that cover $X$, then there exists an index $i_0$ such that 
        \begin{equation*}
            \nu(E_{i_0})\gtrsim\frac{\nu(X)}{i_0^2}
        \end{equation*}
    \end{lemma}
    We will use this lemma, and essentially the $i_0$ in here to argue that there are many $U_j$'s with $diam(U_j)\sim 2^{-i_0}$.

    Let $\{U_j\}$ be a cover of $E$ with diam at most 1. And let 
    \begin{equation*}
        I_i=\{j\in\mathbb{N}: 2^{-i}<diam(U_j)<2^{-i+1}\} \text{ picking out the indices with a specific diam}
    \end{equation*}
    Let 
    \begin{equation*}
        A_i=\bigcup_{j\in I_j}U_j
    \end{equation*}
    Hence now $A_i$ is the set of $\{U_j\}'s$ with diam $\sim 2^{-i}$. If we fix $e\in S^1$, then $\{A_i\cap I_e\}$ cover $I_e$, where $I_e$ is the unit line segment parallel to $e$, then there exists an index $i_e$ such that 
    \begin{equation*}
        H^1(A_{i_e}\cap I_e)\gtrsim\frac{H^1(I_e)}{i_e^2}=\frac{1}{i_e^2}
    \end{equation*}
    We shall now apply a second pigenholing argument. Let 
    \begin{equation*}
        S_i=\{e\in S^1: i_e=i\}
    \end{equation*}
    where for any $e\in S^1$, $i_e$ is the index that ``captures more weight,'' hence we would like to categorize all directions of $S^1$,
    \begin{equation*}
        S_2=\{e\in S^1: i_e=2\}, S_4=\{e\in S^1: i_e=4\}
    \end{equation*}
    Then clearly $\{S_i\}_{i=1}^\infty$ is a cover of $S^1$, hence again by the pigenhole lemma, there exists an index $i_0$, such that 
    \begin{equation*}
        H^1(S_{i_0})\gtrsim\frac{1}{i_0^2}
    \end{equation*}
    Now let $\delta=2^{-i_0}$, then we have 
    \begin{equation*}
        H^1(S_{i_0})\gtrsim\frac{1}{(\log(1/\delta))^2}
    \end{equation*}
    We now find a maximal set of $S_{i_0}$, $\{e_1, ..., e_N\}$, and 
    \begin{equation*}
        N\gtrsim\frac{H^1(S_{i_0})}{\delta}\gtrsim\delta^{-1}(\log(1/\delta))^{-2}
    \end{equation*}
    Then let $T_k$ be $\delta\times 1$ tube parallel to $e_k$ in $S_{i_0}$. Then for each $j\in I_{i_0}$, we have $diam(U_j)\sim 2^{-i_0}$, hence pick some $x_j\in U_j$,
    \begin{equation*}
        U_j\subset B(x_j, 10\delta), \delta=2^{-i_0}
    \end{equation*}
    If we consider 
    \begin{equation*}
        T_k'=T_k\cap \bigcup_{j\in I_{i_0}}B(x_j, 10\delta)
    \end{equation*}
    We have 
    \begin{equation*}
        |T_k'|\gtrsim |I_{i_0}|\delta^2
    \end{equation*}
    where $|I_{i_0}|$ is the size of the maximal $\delta$-separated set in $S_{i_0}$, hence $|I_{i_0}|\gtrsim \delta^{-1}(\log(1/\delta))^{-2}$, hence 
    \begin{equation*}
        |T_k'|\gtrsim\frac{\delta}{(\log(1/\delta))^{2}}
    \end{equation*}
    Then if we define
    \begin{equation*}
        f=\sum_{k=1}^N\chi_{T_k'}
    \end{equation*}
    Then by bounding the $L^2$ norm of the minkowski bound, we have $\|f\|_{L^2}^2\lesssim N\delta\log(1/\delta)$, thus 
    \begin{equation*}
        \left|\bigcup_{k=1}^NT_k'\right|\gtrsim \frac{\|f\|_{L^1}^2}{\|f\|_{L^2}^2}\gtrsim\frac{1}{\log^9(1/\delta)}
    \end{equation*}
    And given that $\bigcup T_k'$ is covered with $|I_{i_0}|$ balls of size $\delta^2$, we have 
    \begin{equation*}
        |I_{i_0}|\gtrsim\frac{\delta^{-2}}{\log^9(1/\delta)}
    \end{equation*}
    Then to lower bound the quantity $\sum diam(U_j)^s$, for any $s<2$, we simply consider the subportion with $diam(U_j)\sim 2^{-i_0}$, 
    \begin{equation*}
        \sum_{j=1}^\infty diam(U_j)^s\gtrsim |I_{i_0}|2^{-i_0s}\gtrsim \frac{2^{i_0(2-s)}}{i_0^9}\gtrsim 1
    \end{equation*}
    for any $s<2$, as $i_0\to\infty$.
\end{proof}
\qed


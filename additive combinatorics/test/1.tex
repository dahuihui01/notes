\chapter{Probabislitic Methods}



\section{First Moment Method}
The first moment method upperbounds the probabilty of events occuring using expected value.
\begin{warn}
    The first moment method does not give a lower bound.
\end{warn}

If we want to show $A$ contains a subset $B$ that satisfies property $\mathcal{P}$, then it suffices to show that a randomly chosen subset $B$ satisfies $\mathcal{P}$ with positive probability.

\begin{defn}[expected value, variance]
    Let $X$ be a real-valued random variable with discrete support, then its expected value is 
    \begin{equation*}
        \E(X)=\sum_x xP(X)
    \end{equation*}
    And its variance is given by 
    \begin{equation*}
        Var(X)=\E(|X-\E(X)|^2)=\E(X^2)-{\E(X)}^2
    \end{equation*}
\end{defn}
If we let $I(E)$ denote the indicator function on $E$, $I(E)=\begin{cases} 1, E\text{ happens }\\ 0, E\text{ doesn't happen} \end{cases}$ Then $\E(I(E))=P(E)$. Let $F$ be an event with nonzero probability.

\begin{defn}[conditional probability and expected value]
    The conditional probability of event $E$ with respect to $F$ is 
    \begin{equation*}
        P(E\vert F)=\frac{P(E\wedge F)}{P(F)}
    \end{equation*}
    We define conditional expected value as follows
    \begin{equation*}
        \E(X\vert F)=\frac{\E(XI(F))}{\E(I(F))}=\sum_x xP(X=x\vert F)
    \end{equation*}
\end{defn}
A random variable is called Boolean if it only takes values $\{0,1\}$, equivalently, it is the indicator function of some event $E$.



\begin{prop}[Borel-Cantellilemma]
    Let $E_1, \ldots, E_n$ be a sequence of events (possibly infinite), such that $\sum_n P(E_n)<\infty$, then 
    \begin{equation*}
        P(\text{ fewer than $M$ events happen})\geq 1-\frac{\sum_n P(E_n)}{M}
    \end{equation*}
\end{prop}
\begin{proof}
    By moving terms, we show the probability of more than $M$ events happen is $\leq\frac{\sum_n P(E_n)}{M}$. By Markov's inequality, 
    \begin{equation*}
        P(\sum_n I(E_n)\geq M)\leq \frac{\E(\sum_n I(E_n))}{M}=\frac{\sum_nP(E_n)}{M}
    \end{equation*}
    Note that in the last equality, we need the assumption there are finitely many events, which we can do by monotone convergence $\sum_nP(E_n)<\infty$.
\end{proof}

\section{Second Moment Method}
The second moment method tells us that $X$ cannot deviate from the expected value $\E(X)$ too much, and these tools are known as large deviation inequalities.
\begin{thm}[Chebyshev's inequality]
    Let $X$ be a random variable, then for any $\lambda>0$, we have 
    \begin{equation*}
        P(|X-\E(X)|>\lambda Var(X)^{1/2})\leq\frac{1}{\lambda^2}
    \end{equation*}
\end{thm}
\begin{proof}
    If $Var(X)=0$, then $X=\E(X)$, hence the inequality satisfies. Let $Var(X)>0$, we note 
    \begin{equation*}
        P(|X-\E(X)|>\lambda Var(X)^{1/2})=P(|X-\E(X)|^2>\lambda^2 Var(X))
    \end{equation*}
    And by Markov's inequality, we have 
    \begin{equation*}
        P(|X-\E(X)|^2>\lambda^2 Var(X))\leq\frac{\E(|X-\E(X)|^2)}{\lambda^2Var(X)}=\frac{1}{\lambda^2}
    \end{equation*}
\end{proof}
From this, we know that $X=E(X)+O(Var(X)^{1/2})$ with high probability. And $Var(X)=\E(|X-\E(X)|^2)$, then we know $|X-\E(X)|^2\geq Var(X)$ with postiive probability. The tools using variance is the second moment method.
\begin{warn}
    The second moment method gives two-sided bounds on how $X$ distributes, i.e., the two tails decay at $1/\lambda^2$.
\end{warn}
Recall the linearity of expected value is as follows:
\begin{equation*}
    \E\left(\sum_n X_n\right)=\sum_n \E(X_n)
\end{equation*}
But this is not true for variance in general! If $\{X_j\}$ are pairwise independent random variables, then we have 
\begin{equation*}
    Var\left(\sum_n X_n\right)=\sum_nVar(X_n)
\end{equation*}
\begin{lem}
    Let $X_j$ be random variables, then we have 
    \begin{equation*}
        Var\left(\sum_n X_n\right)=\sum_n Var(X_n)+\sum_{i,j, i\neq j}Cov(X_i, X_j)
    \end{equation*}
    where $Cov(X_i, X_j)=\E(X_iX_j)-\E(X_i)\E(X_j)$.
\end{lem}
Let's do an example.
\begin{example}
    Let $B$ be a randomly chosen set in $A$, then we have 
    \begin{equation*}
        Var(|B|)=\sum_a P(a\in B)-P(a\in B)^2
    \end{equation*}
\end{example}
\begin{proof}
    We have $|B|=\sum_a1_B(a)$, and $a_1, a_2\in B$ are pairwise independent events. Then we have 
    \begin{equation*}
        Var(|B|)=\sum_a Var(1_B(a))
    \end{equation*}
    And $Var(1_B(a)))=\E(1_B(a)^2)-\E(1_B(a))^2$, hence $Var(|B|)=\sum_a P(a\in B)-P(a\in B)^2$.
\end{proof}
We note that $Var(|B|)\leq\E(|B|)$.


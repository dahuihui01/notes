\documentclass[openany]{book}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amsthm, amssymb}
\usepackage{yhmath}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{tikz-cd}
\usepackage{quiver}
\renewcommand{\familydefault}{ppl}
\newcommand{\tr}{\text{tr}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\colim}{\text{colim}}
\DeclareMathOperator{\im}{im}
\let\oldemptyset\emptyset
\let\emptyset\varnothing
\newcommand{\tor}{\text{Tor}}
\newcommand{\id}{\text{id}}
\newcommand{\ext}{\text{Ext}}
\newcommand{\ptop}{\text{PTop}}
\newcommand{\pt}{\text{pt}}
\newcommand{\ach}{\text{Ach}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\gal}{\text{Gal}}


\input{hui.tex}

\title{Aluffi Problems}

\date{\today}
\author{Hui Sun}


\begin{document}

\maketitle

\tableofcontents
\newpage

\chapter{Category Theory}

\chapter{Groups I}



\chapter{Rings and Modules}



\chapter{Groups II}


\chapter{Irreducibility of polynomials}

\chapter{Linear Algebra I}




\begin{prob}[6.10]
    Let \( F_1, F_2 \) be free \( R \)-modules of finite rank, and let \( \alpha_1 \), resp., \( \alpha_2 \), be linear transformations of \( F_1 \), resp., \( F_2 \). Let \( F = F_1 \oplus F_2 \), and let \( \alpha = \alpha_1 \oplus \alpha_2 \) be the linear transformation of \( F \) restricting to \( \alpha_1 \) on \( F_1 \) and \( \alpha_2 \) on \( F_2 \).

    \begin{itemize}
        \item Prove that \( P_\alpha(t) = P_{\alpha_1}(t)P_{\alpha_2}(t) \). That is, the characteristic polynomial is multiplicative under direct sums.
    
        \item Find an example showing that the minimal polynomial is not multiplicative under direct sums.
    \end{itemize}
\end{prob}


\begin{prob}[6.13]
    Let $A$ be a square matrix with integer entries. Prove that if $\lambda$ is a rational eigenvalue, then $\lambda\in\mathbb{Z}$.
\end{prob}

\begin{proof}
    Let $p(t)=a_0+a_1t+\dots+a_nt^n$ be the characteristic polynomial of $A$, then $p(\lambda)=0$, letting $\lambda=\frac{p}{q}$, then 
    \begin{equation*}
        p\mid a_0,\quad q\mid a_n
    \end{equation*}
    we know that $p$ is monic, thus $a_n=1$, hence $\lambda\in\Z$.
\end{proof}

\begin{prob}[7.3]
    Prove that two linear transformations of a vector space of dimension \(\leq 3\) are similar if and only if they have the same characteristic and minimal polynomials. Is this true in dimension \(4\)? [ยง6.2]
\end{prob}


\begin{prob}[7.4]
    Let \(k\) be a field, and let \(K\) be a field containing \(k\). Two square matrices \(A, B \in M_n(k)\) may be viewed as matrices with entries in the larger field \(K\). Prove that \(A\) and \(B\) are similar over \(k\) if and only if they are similar over \(K\).
\end{prob}

\begin{prob}[7.7]
    Let \( V \) be a \( k \)-vector space of dimension \( n \), and let \( \alpha \in \text{End}_k(V) \). Prove that the minimal and characteristic polynomials of \( \alpha \) coincide if and only if there is a vector \( v \in V \) such that  
\[ v, \alpha(v), \dots, \alpha^{n-1}(v) \] 
is a basis of \( V \).
\end{prob}


\begin{prob}[7.8]
    Let \( V \) be a \( k \)-vector space of dimension \( n \), and let \( \alpha \in \text{End}_k(V) \). Prove that the characteristic polynomial \( P_\alpha(t) \) divides a power of the minimal polynomial \( m_\alpha(t) \).
\end{prob}
\begin{proof}
    Assume that $k$ is algebraically closed, and polynomials factors, the minimal polynomial $m_\alpha$ contains all the $(t-\lambda_i)$ for distinct $\lambda_i$'s by Lemma 7.12. Thus $P_\alpha$ divides $(m_\alpha)^n$.
\end{proof}

\begin{prob}[7.12]
    Let \( V \) be a finite-dimensional \( k \)-vector space, and let \( \alpha \in \text{End}_k(V) \) be a diagonalizable linear transformation. Assume that \( W \subseteq V \) is an invariant subspace, so that \( \alpha \) induces a linear transformation \( \alpha|_W \in \text{End}_k(W) \). Prove that \( \alpha|_W \) is also diagonalizable. (Use Proposition 7.18.)
\end{prob}
\begin{proof}
    Assume that characteristic polynomial factors completely over $k$, then $\alpha$ is diagonalizable iff minimal polynomial $m_\alpha$ has no repeated roots, thus $\alpha\vert_W$ also has no repeated roots as it divides $m_\alpha$.
\end{proof}


\begin{prob}[7.13]
    Let \( R \) be an integral domain. Assume that \( A \in \mathcal{M}_n(R) \) is diagonalizable, with distinct eigenvalues. Let \( B \in \mathcal{M}_n(R) \) be such that \( AB = BA \). Prove that \( B \) is also diagonalizable, and in fact it is diagonal w.r.t. a basis of eigenvectors of \( A \). (If \( P \) is such that \( PAP^{-1} \) is diagonal, note that \( PAP^{-1} \) and \( PBP^{-1} \) also commute.)
\end{prob}
\begin{proof}
    It suffices to see that if $v_1\neq 0$ is such that $Av_1=\lambda_1v_1$, then
    \begin{align*}
        A(Bv_1)&=B(Av_1)\\
        &=B\lambda_1v_1\\
        &=\lambda_1(Bv_1)
    \end{align*}
    Thus $Bv_1$ is contained in the one-dimensional subspace generated by $v_1$.
\end{proof}


\begin{prob}[7.14]
    Prove that "commuting transformations may be simultaneously diagonalized", in the following sense. Let \( V \) be a finite-dimensional vector space, and let \( \alpha, \beta \in \text{End}_k(V) \) be diagonalizable transformations. Assume that \( \alpha\beta = \beta\alpha \). Prove that \( V \) has a basis consisting of eigenvectors of both \( \alpha \) and \( \beta \). (Argue as in Exercise 7.13 to reduce to the case in which \( V \) is an eigenspace for \( \alpha \); then use Exercise 7.12.)
\end{prob}
\begin{proof}
    Separate into eigenspaces: consider eigenspace $E_1$ of $\alpha$, then diagonalize $\beta$ in $E_1$ (by 7.12), note that $E_1$ is invariant under $\beta$.
\end{proof}

\begin{prob}[7.15]
    A \textbf{complete flag} of subspaces of a vector space \( V \) of dimension \( n \) is a sequence of nested subspaces
\[
0 = V_0 \subsetneq V_1 \subsetneq \cdots \subsetneq V_{n-1} \subsetneq V_n = V
\]
with \(\dim V_i = i\). In other words, a complete flag is a composition series in the sense of Exercise 1.16.
\end{prob}





\begin{prob}[7.17]
    A matrix \( M \in M_n(\mathbb{C}) \) is \textbf{normal} if \( MM^\dagger = M^\dagger M \). Note that unitary matrices ($UU^*=U^*U=I$) and Hermitian matrices ($U=U^*$) are both normal. Prove that a triangular normal matrix is diagonal. [7.18]
\end{prob}


























\chapter{Fields}


\chapter{Linear Algebra II}




\end{document}
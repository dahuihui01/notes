


\chapter{Semisimple Algebra}

% Page 19-20

% The most recent semisimple question appeared in 2020.

% (2020, 2017, 2016, 2011, 2010, 2008, 2006)

\begin{defn}[Division ring]
    Any nonzero element in a unit.
\end{defn}
\begin{prop}
    Let $A$ be a semisimple finite-dimensional algebra over $F$, then $A$ can be decomposed into a direct sum of matrix algebras over a division ring:
    \begin{equation*}
        A=M_{n_1}(D_1)\oplus\dots\oplus M_{n_k}(D_k)
    \end{equation*}
    where $D_i$'s are division rings, $M_{n_i}$ is the algebra of $n_i\times n_i$ matrices with entries in $D_i$. This decomposition is unique up to permutation.

    For example, let $G$ be a finite, group, then the group algebra $\C(G)$ can be decomposed to 
    \begin{equation*}
        \C(G)=M_{n_1}(\C)\oplus\dots\oplus M_{n_k}(\C)
    \end{equation*}
    where $|G|=\sum_id_i^2$, and $k$ is the number of conjugacy classes of $G$. Hence it suffices to compute the irreducible representations of $G$.
\end{prop}

\begin{prob}[F2019-Q5]
    Determine the number of two-sided ideals in the group algebra \(\mathbb{C}[S_3]\), where \(S_3\) is the symmetric group of permutations of \(\{1, 2, 3\}\).
\end{prob}
\begin{proof}
    Using the Proposition above, we know that 
    \begin{equation*}
        \C(S_3)=M_1(\C)+M_1(\C)+M_2(\C)=\C\oplus\C\oplus M_2(\c)
    \end{equation*}
\end{proof}



\begin{prob}[F2009-Q6, F2001-Q5]
    Let \( \rho \colon G \to \mathrm{GL}_n(\mathbb{C}) \) be an irreducible complex representation of a finite group \( G \), with character \( \chi \), and let \( C \) be the center of \( G \).
    
    \begin{enumerate}
        \item Prove that for every \( s \in C \), the matrix \( \rho(s) \) is a scalar multiple of the identity matrix \( I_n \).
        
        \item Using part (a), show that \( |\chi(s)| = n \) for all \( s \in C \).
        
        \item Establish the inequality \( n^2 \leq [G : C] \), where \( [G : C] \) is the index of \( C \) in \( G \).
        
        \item Prove that if \( \rho \) is faithful (i.e., injective), then \( C \) must be cyclic.
    \end{enumerate}
\end{prob}



\begin{proof}
    \begin{enumerate}
        \item $\mathbb{C}$ is algebraically closed therefore Schur's lemma applies (see F2017-Q6)
        \item We know that 
        \begin{equation*}
            \rho(z)=\begin{pmatrix}
                \lambda&0&\dots&0\\
                0&\lambda&\dots&0\\
                \dots\\
                0&\dots&0&\lambda
            \end{pmatrix}
        \end{equation*}
        We also know that $C$ is finite and $\rho(z^r)=I$, which implies $|r|=1$. This gives $|\chi(s)|=n$ for all $s\in C$. 
        \item We know that $\rho$ is irreducible, hence the corresponding character $\chi$ satisfies 
        \begin{equation*}
            \frac{1}{|G|}\sum_{g\in G}|\chi(g)|^2=\frac{|C|}{|G|}n^2+\frac{1}{|G|}\sum_{g\not\in C}|\chi(g)|^2=1
        \end{equation*}
        This implies that 
        \begin{equation*}
            \frac{|C|}{|G|}n^2\leq 1\Rightarrow n^2\leq[G:C]
        \end{equation*}
        \item If $\rho$ is faithful, then $C$ embeds into $k^\times$, and any finite subgroup of $k^\times$ is cyclic.
    \end{enumerate}
\end{proof}

\begin{prob}[S2017-Q5]
    Prove directly from the definition of (left) semisimple ring that every such
ring is (left) Noetherian and Artinian. (You may freely use facts about
semisimple, Noetherian, and Artinian modules.)
\end{prob}
\begin{proof}
    Any semisimple ring $R$ can be decomposed into a finite direct sum of simple ideals $I_i$:
    \begin{equation*}
        R=\bigoplus_{i=1}^nI_i
    \end{equation*}
    This directly implies that the ascending and descending chain condition: there aren't infinitely sequence of ideals of strict inclusions.
\end{proof}





\begin{prob}[S2005-Q4]
    Let \( R \) be a ring and \( L \) a minimal left ideal of \( R \) (i.e., \( L \) contains no non-zero proper left ideals of \( R \)). Assuming \( L^2 \neq 0 \), prove that \( L = Re \) for some non-zero idempotent element \( e \in R \).
\end{prob}
\begin{proof}
    We recall that a ring element $e\in R$ is an idempotent if and only if 
    \begin{equation*}
        e^2=e
    \end{equation*}
    It suffices to show that there exists a nonzero idempotent element $e\in L$ since $Re$ is an ideal contained in $L$, thus $Re=L$. Take any $x\neq 0$ in $L$, such that there exists $g\in L$ such that $gx\neq 0$ (this is guaranteed by $L^2\neq 0$). The ideal $Lx$ is contained in $L$, since $L$ is simple, we must have 
    \begin{equation*}
        L=Lx
    \end{equation*}
    Hence $x\in L$ can be written as 
    \begin{equation*}
        x=ex
    \end{equation*}
    for some $e\in L$, multiplying both sides by $e$ and moving terms, we get 
    \begin{equation*}
        (e^2-e)x=0
    \end{equation*}
    It suffices to show that 
    \begin{equation*}
        \left\{g\in L: gx=0\right\}=\{0\}
    \end{equation*}
    This is because $\{g\in L: gx=0\}$ is again an ideal contained in $L$, since we assumed that there exists some $g\in L$ such that $gx\neq 0$, 
    \begin{equation*}
        \{g\in L:gx=0\}=\{0\}
    \end{equation*}
    and we are done!
\end{proof}


\begin{prob}[S2016-Q6, F2006-Q6, F2008-Q6]
    Let \( A \) be a finite-dimensional semisimple algebra over \( \mathbb{C} \), and let \( V \) be an \( A \)-module that decomposes as \( V \cong S \oplus S \), where \( S \) is a simple \( A \)-module. Determine the automorphism group \( \operatorname{Aut}_A(V) \) of \( V \) as an \( A \)-module.
\end{prob}
\begin{proof}
   By Schur's lemma, since $S$ is a simple $A$-module, we know 
   \begin{equation*}
        \text{End}_A(S)\cong\C
   \end{equation*}
   Thus 
   \begin{equation*}
        \text{End}(V)\cong M_2(\C)
   \end{equation*}
   hence 
   \begin{equation*}
        \text{Aut}_A(V)\cong \text{GL}_2(\C)
   \end{equation*}
\end{proof}



\begin{prob}[S2010-Q5]
    Classify all non-commutative semi-simple rings with $512$ elements.
(You can use the fact that finite division rings are fields.)
\end{prob}
\begin{proof}
    By Artin-Wedderburn, we know that this finite semisimple ring can be decomposed into a finite direct sum of matrix rings:
    \begin{equation*}
        R\cong M_{n_1}(F_1)\oplus\dots\oplus M_{n_k}(F_k)
    \end{equation*}
    where $F_i$ are finite fields. Further more we can assume $n_1\geq n_2\geq\dots\geq n_k$.The total number of elements is 
    \begin{equation*}
        F_1^{n_1^2}\dots F_k^{n_k^2}=512=2^9
    \end{equation*}
    Thus we see all the components are powers of $2$. Since $R$ is noncommutative, we may assume that $n_1\geq 2$. If $n_1=3$, then $F_1=2$, we have 
    \begin{equation*}
        R\cong M_{3}(\F_2)
    \end{equation*}
    If $n_1=2$, we can have $n_2=2$, then 
    \begin{equation*}
        R\cong M_2(\F_2)\oplus M_2(\F_2)\oplus \F_2
    \end{equation*}
    or $n_3=\dots=n_k=1$, then we have (different ways of adding to $5$):
    \begin{equation*}
        R\cong M_2(\F_2)\oplus\begin{cases}
            \F_2\oplus\F_2\oplus\F_2\oplus\F_2\oplus F_2\\
            \F_{4}\oplus \F_2\oplus\F_2\oplus F_2\\
            \F_8\oplus\F_2\oplus\F_2\\
            \F_{16}\oplus\F_2\\
            \F_8\oplus\F_4\\
            \F_{16}\oplus F_2\\
            \F_{32}
        \end{cases}
    \end{equation*}



    % There are three semi-simple rings of of order $512$ that are not commutative. They are 
    % \begin{equation*}
    %     \F_2\oplus\F_2\oplus\F_2\oplus\F_2\oplus\F_2\oplus M(\F_4)
    % \end{equation*},
    % \begin{equation*}
    %     \F_2\oplus M(\F_4)\oplus M(\F_4)
    % \end{equation*}
    % and 
    
    % \begin{equation*}
    %     M(\F_9)
    % \end{equation*}
\end{proof}

\begin{prob}[F2011-Q5]
    Let \( A \) be a finite-dimensional semisimple algebra over \( \mathbb{C} \), and let \( V \) be a finitely-generated \( A \)-module. Prove that \( V \) has only finitely many \( A \)-submodules if and only if \( V \) decomposes into a direct sum of pairwise irreducible non-isomorphic (i.e., simple)  \( A \)-modules.
\end{prob}
\begin{proof}
    Suppose that $V$ is a direct sum of distinct irreducible $A$-modules, then 
    \begin{equation*}
        V=S_1\oplus\dots\oplus S_n
    \end{equation*}
    where $S_i$'s are nonisomorphic and simple. Hence the only submodules of $S_i$ is $\{0\}$ and $S_i$, i.e., there are only finitely many submodules of $V$.

    Conversely, we suppose that there are finitely many $A$-submodules of $V$, because $V$ is semisimple, we know 
    \begin{equation*}
        V=\bigoplus_{i=1}^n S_i^{n_i}
    \end{equation*} 
    where $S_i$'s are semisimple. It suffices to show that $n_i=1$ for all $i$. Suppose that 
    \begin{equation*}
        V=S_i\oplus S_i
    \end{equation*}
    By Schur's lemma, we have 
    \begin{equation*}
        \text{Hom}(S_i,S_i)\cong\C
    \end{equation*}
    there are infinitely many distinct $\phi:S_i\to S_i$, and we note that 
    \begin{equation*}
        \left\{(s,\phi(s)):\phi\in\text{Hom}(S_i,S_i)\right\}
    \end{equation*} 
    is a submodule of $V$, thus there are infinitely many submodules, which is a contradiction.
\end{proof}






















\chapter{Linear Algebra I}

Topics: finitely generated modules/PID, triangularization, diagonalization, Jordan canonical form.

% Page 21-23

\begin{prop}
    A linear operator $T:V\to V$ is diagonalizable if and only if the minimal polynomial splits into distinct linear factors (has no repeated roots).
\end{prop}


\begin{prob}[F2018-Q1]
    Let \( V \) be an \( n \)-dimensional vector space over a field \( k \) and let \( \alpha: V \to V \) be a linear endomorphism.  
Prove that the minimal and characteristic polynomials of \( \alpha \) coincide if and only if there is a vector \( v \in V \) so that:  

\[\{v, \alpha(v), \ldots, \alpha^{n-1}(v)\}\]

is a basis for \( V \).
\end{prob}
\begin{proof}
    We will show there is not vector $v\in V$ such that $\{v, \alpha(v), \ldots, \alpha^{n-1}(v)\}$ is linearly independent if and only if the minimal and characteristic polynomials aren't the same. 
    \begin{align*}
        \text{no $v$ s.t. $\{v,\dots, \alpha^{n-1}(v)\}$ is linearly independent}&\iff \text{ for all } v\in V, \{v,\ldots, \alpha^{n-1}v\} \text{is linearly dependent}\\
        &\iff \alpha^{n-1}v=c_0v+\dots+c_{n-2}\alpha^{n-2}v \text{ for all } v
    \end{align*}
    We then claim that $\alpha^{n-1}v=c_0v+\dots+c_{n-2}\alpha^{n-2}v \text{ for all } v$ if and only if the minimal polynomial $p_m$ has degree less than the characteristic polynomial $p_n$. Note that $p_n$ has degree $n$, and we can write 
    \begin{equation*}
        p_n(t)=a_nt^n+\dots+a_1t+a_0
    \end{equation*}
    and $p_n(\alpha)v=0$ for all $v\in V$, therefore $\alpha^{n-1}v=c_0v+\dots+c_{n-2}\alpha^{n-2}v \text{ for all } v$ if and only if 
    \begin{equation*}
        p_n(\alpha)v=a_n\alpha^{n-1}v+\dots+a_1\alpha v+a_0=p_{n-1}(\alpha)v=0
    \end{equation*}
    where $p_{n-1}$ is a polynomial of degree at most $n-1$ and is such that $p_{n-1}(\alpha)=0$. Thus no $v$ such that $\{v,\dots,\alpha^{n-v}v\}$ is linearly independent if and only if minimal and characteristic polynomial have different degrees.
\end{proof}


\begin{prob}[F2018-Q3]
    \phantom{text}
    \begin{itemize}
        \item[(a)] Fix a positive integer \( n \) and classify all finite modules over the ring \( \mathbb{Z}/n\Z \).
        \item[(b)] Prove, either using (a) or from first principles, for a fixed prime \( p \) that all finite modules over \( \mathbb{Z}/p\Z \) are free.
    \end{itemize}
\end{prob}
\begin{proof}
    \textcolor{red}{not finished}
\end{proof}


\begin{prob}[F2017-Q2]
    Let \(\Lambda\) be a free abelian group of finite rank \(n\), and let \(\Lambda' \subset \Lambda\) be a subgroup of the same rank. Let \(x_1, \ldots, x_n\) be a \(\mathbb{Z}\)-basis for \(\Lambda\), and let \(x_1', \ldots, x_n'\) be a \(\mathbb{Z}\)-basis for \(\Lambda'\). For each \(i\), write \(x_i' = \sum_{j=1}^n a_{ij}x_j\), and let \(A := (a_{ij}) \in \operatorname{Mat}_{n \times n}(\mathbb{Z})\). Show that the index \([\Lambda : \Lambda']\) equals \(|\det A|\).
\end{prob}
\begin{proof}
    Up to some basis change, we can write 
    \begin{equation*}
        \Gamma'=d_1\Z\oplus\dots\oplus d_k\Z
    \end{equation*}
    given $\Gamma=\Z\oplus\dots\oplus\Z$. Then Since we are taking the determinant, it is invariant under change of basis. One can compute the matrix using the standard basis for $\Gamma$ and $\Gamma'$, and it is clear that $[\Gamma:\Gamma']=\prod_{i=1}^kd_i=|\det(A)|$.
\end{proof}


\begin{prob}[S2001-Q5]
    \phantom{text}
    \begin{itemize}
        \item[(a)] Prove that an \( n \times n \) matrix \( A \) with entries in the field \( \mathbb{C} \) of complex numbers, satisfying \( A^3 = A \), can be diagonalized over \( \mathbb{C} \).
        
        \item[(b)] Does the statement in (a) remain true if one replaces \( \mathbb{C} \) by an arbitrary algebraically closed field \( F \)? Why or why not?
    \end{itemize}
\end{prob}
\begin{proof}
    \begin{itemize}
        \item[(a)] $A$ is diagonalizable if and only if the minimal polynomial splits into distinct linear factors. The characteristic polynomial is $p(t)=t(t+1)(t-1)$ and the minimal polynomial $p_m\mid p$ thus $A$ is diagonalizable.
        \item[(b)] This is not true. Take $k$ to be a field of characteristic $2$, then 
        \begin{equation*}
            p(t)=t(t^2-1)=t(t-1)^2
        \end{equation*}
        Thus the minimal polynomial could be $(t-1)^2$, i.e., $A$ is not necessarily diagonalizable.
    \end{itemize}
\end{proof}



\begin{prob}[F2001-Q3]
    Let \( A \) be an \( n \times n \) complex matrix with \( A^m = 0 \) for some integer \( m > 0 \).
    
    \begin{enumerate}
        \item Show that if \( \lambda \) is an eigenvalue of \( A \), then \( \lambda = 0 \).
        
        \item Determine the characteristic polynomial of \( A \).
        
        \item Prove that \( A^n = 0 \).
        
        \item Construct a \( 5 \times 5 \) matrix \( B \) satisfying \( B^3 = 0 \) but \( B^2 \neq 0 \).
        
        \item For any \( 5 \times 5 \) complex matrix \( M \) with \( M^3 = 0 \) and \( M^2 \neq 0 \), is \( M \) necessarily similar to your matrix \( B \) from part (d)? Justify your answer.
    \end{enumerate}
\end{prob}
\begin{enumerate}
    \item Suppose $\lambda$ is an eigenvalue, then there exists $v\neq 0$, such that 
    \begin{equation*}
        A^mv=\lambda^mv=0\Rightarrow \lambda=0
    \end{equation*}
    \item The characteristic polynomial is $p(t)=t^n$.
    \item Cayley-Hamilton theorem.
    \item Can have 
    \begin{equation*}
        B=\begin{pmatrix}
            0&1&0&0&0\\
            0&0&1&0&0\\
            0&0&0&0&0\\
            0&0&0&0&0\\
            0&0&0&0&0
        \end{pmatrix}
    \end{equation*}
    The important is that the top left $3\times 3$ matrix $A$ satisfies $A^3=0, A^3\neq 0$. This is constructed by building $B$ using the Jordan form.
    \item No, the lower $2\times 2$ matrix could be 
    \begin{equation*}
        \begin{pmatrix}
            0&0\\
            0&0
        \end{pmatrix}\quad \text{ or }\quad \begin{pmatrix}
            0&1\\
            0&0
        \end{pmatrix}
    \end{equation*}
\end{enumerate}


\begin{prob}[F2018-Q4]
    In this question all modules are left modules.

Let \( k \) be a field of characteristic different from 2 and let \( G = \{e, g\} \) be the multiplicative group with two elements. Consider the group ring \( A = k[G] \).

\begin{enumerate}
    \item[(a)] Show that the \( A \)-module \( A \) is a direct sum of two ideals of \( A \).
    \begin{itemize}
        \item List all proper ideals of \( A \).
        \item Is \( A \) a principal ideal domain?
    \end{itemize}
    
    \item[(b)] Show that every \( A \)-module decomposes into a direct sum of simple \( A \)-modules.
    
    \item[(c)] Assume now that the characteristic of \( k \) is 2. Give an example of an \( A \)-module that cannot be decomposed into a direct sum of two simple \( A \)-modules.
\end{enumerate}
\end{prob}
\begin{proof}
    \textcolor{red}{not finished}
\end{proof}


\begin{prob}[S2003-Q3]
    Prove that if a linear operator on a complex vector space is diagonal in some basis, then its restriction to any invariant subspace \( L \) is also diagonal in some basis of \( L \).
\end{prob}
\begin{proof}
    The linear operator $T$ is diagonalizable if and only if the minimal polynomial has no repeated factors, i.e., 
    \begin{equation*}
        f_m(x)=(x-\lambda_1)\dots(x-\lambda_k)
    \end{equation*}
    And $T\vert_L$ has minimal polynomial dividng $f_m$, hence it also has no repeated factors, thus $T\vert_L$ is also diagonalizable.
\end{proof}


% \begin{center}
%     \textcolor{pink}{End of Page 22}
% \end{center}


\begin{prob}[S2017-Q4]
    Let \(M\) be an invertible \(n\times n\) matrix with entries in an algebraically closed field \(k\) of characteristic not 2. Show that \(M\) has a square root, i.e. there exists \(N\in\text{Mat}_{n\times n}(k)\) such that \(N^{2}=M\).
\end{prob}
\begin{proof}
    It suffices to show that every Jordan block 
    \[
        J_n(\lambda) = 
        \begin{bmatrix}
        \lambda & 1       & 0       & \cdots & 0 \\
        0       & \lambda & 1       & \cdots & 0 \\
        0       & 0       & \lambda & \ddots & \vdots \\
        \vdots  & \vdots  & \ddots  & \ddots & 1 \\
        0       & 0       & \cdots  & 0      & \lambda
        \end{bmatrix}
        \]
        where $\lambda\neq 0$ is a square. We will proceed using inductino. When $n=2$, the square root of 
        \begin{equation*}
            \begin{bmatrix}
                \lambda&1\\
                0&\lambda
            \end{bmatrix}=\begin{bmatrix}
                \lambda^\frac{1}{2}&\frac{1}{2}\lambda^{-\frac{1}{2}}\\
                0&\lambda^\frac{1}{2}
            \end{bmatrix}^2
        \end{equation*} 
        Now assume that $J_{k}$ is a square up to $k=n-1$, we want to show $J_n$ also has a square root. We claim $J_n$ has the following square 
        \begin{equation*}
            J_n=\begin{bmatrix}
                B^2&x\\
                0&\lambda

            \end{bmatrix}=\begin{bmatrix}
                B&x\\
                0&\lambda^{1/2}
            \end{bmatrix}^2
        \end{equation*}
        where $B$ is a $(n-1)\times (n-1)$ matrix and $x=(x_1,\dots,x_{n-1}), 0=(0,\dots,0)$. It suffices to find such an $x$ exists. Let $b_1,\dots,b_{n-1}$ denote the row vectors of $B$, we must satisfy 
        \begin{equation*}
            \begin{cases}
                b_1\cdot x+x_1\lambda^\frac{1}{2}=0\\
                \dots\\
                b_{n-2}\cdot x+x_{n-2}\lambda^\frac{1}{2}=0\\
                b_{n-1}\cdot x+x_{n-1}\lambda^\frac{1}{2}=1\\
            \end{cases}
        \end{equation*}
        Namely, we need to find $x$ that satisfies 
        \begin{equation*}
            (B+\lambda^\frac{1}{2}I)x=\begin{bmatrix}
                0\\
                \dots\\
                0\\
                1
            \end{bmatrix}
        \end{equation*}
        Since $(B+\lambda^{1/2}I)$ is invertible, there exists a unique solution, hence such $x$ exsits, $J_n$ has a square root! 
\end{proof}



\begin{prob}[S2008-Q1]
    Let \(k\) be a field. Consider the subgroup \(B \subset \text{GL}_2(k)\) where
    \[B = \left\{ \begin{pmatrix} a & b \\ 0 & d \end{pmatrix} \mid a, b, d \in k, ad \neq 0 \right\}.\]
    \begin{itemize}
        \item[(a)] Let \(Z\) be the center of \(\text{GL}_2(k)\). Show that
        \[\bigcap_{x \in \text{GL}_2(k)} x^{-1} Bx = Z.\]
        \item[(b)] Assume \(k\) is algebraically closed. Show that
        \[\bigcup_{x \in \text{GL}_2(k)} x^{-1} Bx = \text{GL}_2(k).\]
        \item[(c)] Assume \(k\) is a finite field. Can the statement in (b) still be true?
    \end{itemize}
\end{prob}
\begin{proof}
    \begin{itemize}
        \item[(a)] Let $y\in \bigcap_{x \in \text{GL}_2(k)}$, then for all $x\in\text{GL}_2(k)$, we have $xyx^{-1}\in B$. This shows that 
        \begin{align*}
            xyx^{-1}\in B\text{ for all }x&\iff xyx^{-1}\begin{bmatrix}
                0\\
                1
            \end{bmatrix}\in\left\la\begin{bmatrix}
                0\\
                1
            \end{bmatrix}\right\ra\\
            &\iff x^{-1}\begin{bmatrix}
                0\\
                1
            \end{bmatrix}
            \text{ is a subspace for } y \text{ for all }x\\
            &\iff \text{ the whole vector space is the eigenspace of }y\\
            &\iff \text{ $y$ is a scalar}\\
            &\iff y\in Z  
        \end{align*}
        \item[(b)] If $k$ is algebraically closed, then any matrix can be written as a triangular matrix up to some basis change.
        \item[(c)] (If $k$ is $R$, the statement of (b) is also wrong). It's the same idea for finite groups, one can take $g\in\overline{\mathbb{F}_p}\setminus\mathbb{F}_p$, then the characteristic polynomial for the map of multiplication by $g:\overline{\mathbb{F}_p}\to \overline{\mathbb{F}_p}$ where $\overline{\mathbb{F}_p}=\mathbb{F}_{p^2}$ is a vector space over $\mathbb{F}$ the minimial polynomial is $(t-g)^2$ which is irreducible over $\mathbb{F}_p$.
    \end{itemize}
\end{proof}

\begin{prob}[S2009-Q4]
    Let \(E\) be a finite-dimensional vector space over an algebraically closed field \(k\). Let \(A, B\) be \(k\)-endomorphisms of \(E\). Assume \(AB = BA\). Show that \(A\) and \(B\) have a common eigenvector.
\end{prob}
\begin{proof}
    Since $k$ is algebraically closed, we know there exists at least one eigenvector of $A$, i.e., there exists $\lambda$ such that $Av=\lambda v$ for some $v\neq 0$. We denote this eigenspace by $E_\lambda$, and we note that $E_\lambda$ is invariant under $B$: let $v\in E_\lambda$
    \begin{equation*}
        A(Bv)=\lambda(Bv)
    \end{equation*}
    thus $Bv\in E_\lambda$ as well. Then it suffices to find an eigenvector of $B$ living inside $E_\lambda$, this is done by noting $B\vert_{E_\lambda}$ has an eigenvector in $E_\lambda$, as desired.
\end{proof}


\begin{prob}[F2005-Q6]
    Let \(E\) be a finite-dimensional vector space over a field \(k\). Assume \(S, T \in \text{End}_k(E)\). Assume \(ST = TS\) and both of them are diagonalizable. Show that there exists a basis of \(E\) consisting of eigenvectors for both \(S\) and \(T\).
\end{prob}
\begin{proof}
    It is the same proof as above except now we do this for all $E_{\lambda_1},\dots, E_{\lambda_k}$.
\end{proof}

\begin{prob}[S2015-Q2]
    Let \(A,B\) be two commuting operators on a finite dimensional space \(V\) over \(\mathbb{C}\) such that \(A^{n}=B^{m}\) is the identity operator on \(V\) for some positive integers \(n,m\). Prove that \(V\) is a direct sum of 1-dimensional invariant subspaces with respect to \(A\) and \(B\) simultaneously.
\end{prob}
\begin{proof}
    Because 
    \begin{equation*}
        A^n=B^m=I
    \end{equation*}
    We know that the minimal polynomial of $A,B$ both have no repeated roots, because $(t^n-1), (t^m-1)$ factor completely over $\C$. This shows that $A,B$ are commuting diagonalizable matrices, thus they can be simultaneously diagonalized.
\end{proof}






\chapter{Linear Algebra II}
Topics: exterior power, tensor algebras, trances, determinants

% Page 24-25

\begin{prob}[F2016-Q5]
    Let A be a linear transformation of a finite dimensional vector space over a field of characteristic \(\neq 2\).
    \begin{itemize}
        \item[(1)] Define the wedge product linear transformation \(\wedge^{2}A=A\wedge A\).
        \item[(2)] Prove that
        \[tr(\wedge^{2}A)=\frac{1}{2}(tr(A)^{2}-tr(A^{2})).\]
    \end{itemize}
\end{prob}
\begin{proof}
    \begin{itemize}
        \item[(a)] We recall the wedge product of vector space $V\wedge V$ is given by the basis 
        \begin{equation*}
            \{v_i\wedge v_j: i<j\}
        \end{equation*}
        satisfying 
        \begin{equation*}
            v_i\wedge v_j=-v_j\wedge v_i
        \end{equation*}
        where $\{v_1,\dots, v_n\}$ is a basis for $V$. And we define 
        \begin{equation*}
            A\wedge A(v_i\wedge v_j)=Av_i\wedge A_j
        \end{equation*}
        \item[(b)] Consider the matrix representation of $A=(A_{ij})$, on the basis $\{v_i\wedge v_j: i<j\}$, 
        \begin{align*}
            A\wedge A(v_i\wedge v_j)&=\sum_{k,l=1}^nA_{ki}A_{lj}(v_k\wedge v_l)\\
            &=\sum_{k<l}A_{ki}A_{lj}(v_k\wedge v_l)+\sum_{l<k}A_{ki}A_{lj}(v_k\wedge v_l)\\
            &=\sum_{k<l}A_{ki}A_{lj}(v_k\wedge v_l)-\sum_{l<k}A_{ki}A_{lj}(v_l\wedge v_k)
        \end{align*}
        Thus the diagonal term with respect to $v_i\wedge v_j$ is 
        \begin{equation*}
            A_{ii}A_{jj}-A_{ji}A_{ij}
        \end{equation*}
        Thus 
        \begin{equation*}
            \text{Tr}(A\wedge A)=\sum_{i<j}A_{ii}A_{jj}-A_{ji}A_{ij}
        \end{equation*}
        Now 
        \begin{equation*}
            \text{Tr}(A)^2=\sum_{i=1}^nA_{ii}^2+2\sum_{i<j}A_{ii}A_{jj}
        \end{equation*}
        and 
        \begin{align*}
            \text{Tr}(A^2)&=\sum_{k,l=1}^nA_{lk}A_{kl}\\
            &=\sum_{i=1}^nA_{ii}^2+2\sum_{k<l}A_{lk}A_{kl}
        \end{align*}
        Thus we see that 
        \[tr(\wedge^{2}A)=\frac{1}{2}(tr(A)^{2}-tr(A^{2}))\]
    \end{itemize}
\end{proof}


\begin{prob}[S2006-Q5]
    Let \(V\) be a finite-dimensional vector space over a field \(k\). Let \(T \in \text{End}_k(V)\). Show that \(\text{tr}(T \otimes T) = (\text{tr}(T))^2\). Here \(\text{tr}(T)\) is the trace of \(T\).
\end{prob}
\begin{proof}
    We will show that $\tr(T\otimes T)=(\tr T)^2$, and the $T\otimes T\otimes$ is done similarly. We will use matrix representation to do an explicit computation. Let $\{v_1,\dots, v_n\}$ be a basis of$V$, then $V\otimes V$ has basis 
        \begin{equation*}
            \{v_i\otimes v_j: 1\leq i,j\leq n\}
        \end{equation*}
        and 
        \begin{equation*}
            T\otimes T(v_i\otimes v_j)=Tv_i\otimes Tv_j
        \end{equation*}
        Let $T=(a_{ij})$, then we know 
        \begin{equation*}
            (\tr(T))^2=\left(\sum_{i=1}^na_{ii}\right)^2
        \end{equation*}
        And we have 
        \begin{align*}
            T\otimes T(v_i\otimes v_j)&=\sum_{k=1}^n\sum_{l=1}^na_{ki}a_{lj}v_k\otimes v_l
        \end{align*}
        Therefore computing the trace we see 
        \begin{equation*}
            \tr(T\otimes T)=\sum_{i=1}^n\sum_{j=1}^na_{ii}a_{jj}=\tr(T)^2
        \end{equation*}
        as desired!
\end{proof}

\begin{prob}[S2016-Q4]
    Let \(V\) and \(W\) be two finite dimensional vector spaces over a field \(K\). Show that for any \(q>0\),
    \[\bigwedge^{q}(V\oplus W)\cong\sum_{i=0}^{q}(\bigwedge^{i}(V)\otimes_{K}\bigwedge^{q-i}(W)).\]
\end{prob}
\begin{proof}
    Any two finite dimensional vector spaces of the same dimension are isomorphic. Hence, it suffices to show that the dimensions are equal. We will convince ourselves it holds for $q=2$. Let $\{v_1,\dots, v_n\}$ be the basis of $V$, and $\{w_1,\dots,w_k\}$ be the basis of $W$, then we begin with the LHS:
    \begin{equation*}
        \bigwedge^{2}(V\oplus W)
    \end{equation*}
    We note that $V\oplus W$ has basis 
    \begin{equation*}
        \left\{(v_i,w_j):1\leq i\leq n, 1\leq j\leq k\right\}
    \end{equation*}
    So we reenumerate the $n+k$ basis as 
    \begin{equation*}
        \{e_1,\dots, e_{n+k}\}
    \end{equation*}
    Then $\bigwedge^{q}(V\oplus W)$ has basis
    \begin{equation*}
        \left\{ e_i\wedge e_j: i<j\right\}
    \end{equation*}
    There are exactly $1+\dots+(n+k-1)$ basis vectors i.e.,
    \begin{equation*}
        \dim\left(\bigwedge^{2}(V\oplus W)\right)=\frac{(n+k-1)(n+k)}{2}
    \end{equation*}
    As for the RHS: 
    \begin{equation*}
        \dim\left( \sum_{i=0}^{2}(\bigwedge^{i}(V)\otimes_{K}\bigwedge^{2-i}(W))\right)
        \frac{(k-1)k}{2}+nk+\frac{(n-1)n}{2}
    \end{equation*}
    And we observe that two two quantities are equal. Now we do the general case, just like above, 
    \begin{equation*}
        \dim\left(\bigwedge^{q}(V\oplus W)\right)=\binom{n+k}{q}
    \end{equation*}
    And the RHS:
    \begin{equation*}
        \dim\left(\bigwedge^{q-1}(V\oplus W)\wedge (V\oplus W)\right)=\sum_{i=0}^q\binom{n}{i}\binom{k}{q-i}
    \end{equation*}
    and it is clear that these two quantities are equal.
\end{proof}



\begin{prob}[S2011-Q4]
    Let \(F\) be a field, and \(V\) a finite-dimensional vector space over \(F\), with \(\dim_F V=n\).
    \begin{itemize}
        \item[(a)] Prove that if \(n>2\), the spaces \(\bigwedge^2(\bigwedge^2(V))\) and \(\bigwedge^4(V)\) are not isomorphic.
        \item[(b)] Let \(k\) be a positive integer. Prove that when \(v\in\bigwedge^k(V)\) and \(0\neq x\in V\), \(v\wedge x=0\) holds if and only if \(v=x\wedge y\) for some \(y\in\bigwedge^{k-1}(V)\).
    \end{itemize}
\end{prob}
\begin{proof}
    \begin{itemize}
        \item[(a)] This is by a dimension argument:
        \begin{equation*}
            \dim\left(\bigwedge^2(\bigwedge^2(V))\right)=\binom{\binom{n}{2}}{2}=\frac{n(n-1)(n-2)(n+1)}{2}
        \end{equation*}
        whereas 
        \begin{equation*}
            \dim\left(\bigwedge^4(V)\right)=\binom{n}{4}=\frac{n(n-1)(n-2)(n-3)}{4}
        \end{equation*}
        Thus not equal if $n>2$.
        \item[(b)] If there exists such $y$ where $v=x\wedge y$, then 
        \begin{equation*}
            v\wedge x=(x\wedge y)\wedge x=(-y\wedge x)\wedge x=0
        \end{equation*}
        Conversely, if $v=0$, then it is immediate that $v=x\wedge x$. It suffices to assume that $v\neq 0$, thus if we write 
        \begin{equation*}
            v=v_1\wedge\dots\wedge v_k
        \end{equation*}
        where $v_i$'s are distinct. Then 
        \begin{equation*}
            v\wedge x=0
        \end{equation*}
        If $v_i=\pm x$ for any $i$, we are done. If not, then we derive a contradiction: $v_1\neq x$, thus 
        \begin{equation*}
            v_1\wedge\left(v_2\wedge\dots\wedge v_k\wedge x\right)=0
        \end{equation*}
        i.e., $v_2\wedge\dots\wedge v_k\wedge x=0$, now $v_2\neq x$, and we keep going, eventually $v_k\wedge x=0$ which implies $x=\pm v_k$. 
    \end{itemize}
\end{proof}


\begin{prob}[S2010-Q4]
    Let \(V\) be a \(n\)-dimensional vector space over a field \(k\). Let \(T \in \text{End}_k(V)\).
    \begin{itemize}
        \item[(a)] Show that \(tr(T \otimes T \otimes T) = (tr(T))^3\). Here \(tr(T)\) is the trace of \(T\).
        \item[(b)] Find a similar formula for the determinant \(\det(T \otimes T \otimes T)\).
    \end{itemize}
\end{prob}
\begin{proof}
    \begin{itemize}
        \item[(a)] The trace computation is exactly the same as the one above. 
        \item[(b)] We can compute via some combinatorics:
        \begin{equation*}
            \det(T\otimes T)=(\det T)^{2n}, \det(T\otimes T\otimes T)=(\det T)^{3n^2}
        \end{equation*}
    \end{itemize}
\end{proof}








\chapter{Linear Algebra III}
Topics: random linear algebra problems

% Page 26-28

\begin{prop}
    Let $V$ be a $m$ dimensional vector space, and $W$ be $n$ dimensional. Show that $A:V\to V$ and $B: W\to W$ has 
    \begin{equation*}
        \text{Tr}(A\otimes B)=\text{Tr}(A)\text{Tr}(B)
    \end{equation*}
\end{prop}
\begin{proof}
    Use matrix representations.
\end{proof}


\begin{prob}[S2013-Q5]
    Let \(A\) and \(B\) be \(n \times n\) matrices with complex coefficients. Assume that \((A - I)^n = 0\) and \(A^k B = BA^k\) for some natural number \(k\). Prove that \(AB = BA\) (\textit{Hint}: Prove that \(A\) can be expressed as a function of \(A^k\)).
\end{prob}
\begin{proof}
    
\end{proof}

\begin{prob}[F2011-Q2]
    Consider the special orthogonal group \(G=SO(3,\mathbb{R})\), namely,
    \[G=\{A\in GL(3,\mathbb{R}): A^T A=\mathrm{I}_3, \det(A)=1\}\]
    \begin{itemize}
        \item[(a)] Show that for any element \(A\) in \(G\), there exists a real number \(\alpha\) with \(-1\leq\alpha\leq 3\) such that
        \[A^3-\alpha A^2+\alpha A-\mathrm{I}_3=0.\]
        \item[(b)] For which real numbers \(\alpha\) with \(-1\leq\alpha\leq 3\) does there exist an element \(A\) in \(G\) whose minimal polynomial is \(x^3-\alpha x^2+\alpha x-1\)? Explain your answer.
    \end{itemize}
\end{prob}
\begin{proof}
    \begin{itemize}
        \item[(a)] The determinant forces the eigenvalues (over $\C$) to have norm $1$. The form is done by explicit computations.
        \item[(b)] It has the minimal polynomial equal to the characteristic polynomial if the polynomial splits into three distinct roots, we know $x=1$ has a root, 
        \begin{equation*}
            (x-1)(x^2+(1-\alpha)x+1)
        \end{equation*}
        Hence as long as $\alpha\neq -1,3$, the minimal polynomial and the characteristic polynomial coincide.
    \end{itemize}
\end{proof}


\begin{prob}[F2007-Q3]
    Let \(A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}\) be a real matrix such that \(a,b,c,d > 0\).
    \begin{itemize}
        \item[(1)] Prove that \(A\) has two distinct real eigenvalues, \(\lambda > \mu\).
        \item[(2)] Prove that \(\lambda\) has an eigenvector in the first quadrant and \(\mu\) has an eigenvector in the second quadrant.
    \end{itemize}
\end{prob}






\begin{prob}[S2007-Q1]
    Prove that the integer orthogonal group \(O_n(\mathbb{Z})\) is a finite group. (By definition, an \(n \times n\) square matrix \(X\) over \(\mathbb{Z}\) is orthogonal if \(XX^t = I_n\).)
\end{prob}

\begin{prob}[F2008-Q4]
    A differentiation of a ring R is a mapping \(D:R\to R\) such that, for all \(x,y\in R\),
    \begin{itemize}
        \item[(1)] \(D(x+y) = D(x) + D(y)\); and
        \item[(2)] \(D(xy) = D(x)y + xD(y)\).
    \end{itemize}
    If \(K\) is a field and \(R\) is a \(K\)-algebra, then its differentiation are supposed to be over K, that is,
    \begin{itemize}
        \item[(3)] \(D(x) = 0\) for any \(x \in K\).
    \end{itemize}
    Let D be a differentiation of the K-algebra \(M_n(K)\) of \(n \times n\)-matrices. Prove that there exists a matrix \(A \in M_n(K)\) such that \(D(X) = AX - XA\) for all \(X \in M_n(K)\).
\end{prob}


\begin{prob}[F2006-Q1]
    Let \(\text{SL}_n(k)\) be the special linear group over a field \(k\), i.e, \(n \times n\) matrices with determinant 1. Let \(I\) be the identity matrix, and \(E_{ij}\) be the elementary matrix that has 1 at \((i,j)\)-entry and 0 elsewhere. Here \(1 \leq i \neq j \leq n\).
    \begin{itemize}
        \item[(1)] Let \(C_{ij}\) be the centralizer of the matrix \(I + E_{ij}\). Find explicit generators of \(C_{ij}\).
        \item[(2)] Find the intersection
        \[\bigcap_{1 \leq i \neq j \leq n} C_{ij}.\]
        \item[(3)] Determine all the elements in the conjugacy class of \(I + E_{ij}\).
    \end{itemize}
\end{prob}


\begin{prob}[S2018-Q1]
    Let \(F\) be a field of characteristic not equal to 2. Let \(D\) be the non-commutative algebra over \(F\) generated by elements \(i,j\) that satisfy the relations
    \[i^2 = j^2 = 1, \quad ij = -ji.\]
    Define \(k = ij.\)
    \begin{itemize}
        \item[(a)] Verify that \(D\) is isomorphic to the algebra \(M_2(F)\) of \(2 \times 2\) matrices in such a way that
        \[1 \leftrightarrow \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}, i \leftrightarrow \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}, j \leftrightarrow \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}, k \leftrightarrow \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}.\]
        \item[(b)] Write \(q = x + yi + zj + uk\) for \(x,y,z,u \in F\). Verify that the norm
        \[N(q) = x^2 - y^2 - z^2 + u^2\]
        corresponds to the determinant under the isomorphism of part (a).
        \item[(c)] What does the involution \(q \mapsto \bar{q} = x - yi - zj - uk\) on \(D\) correspond to on the matrix side?
    \end{itemize}
\end{prob}


\begin{prob}[S2006-Q3]
    Let \(V\) be a \(n\)-dimensional vector space over a field \(k\), with a basis \(\{e_1, \ldots, e_n\}\). Let \(A\) be the ring of all \(n \times n\) diagonal matrices over \(k\). \(V\) is a \(A\)-module under the action:
    \[\text{diag}(\lambda_1, \ldots, \lambda_n) \cdot (a_1 e_1 + \cdots + a_n e_n) = (\lambda_1 a_1 e_1 + \cdots + \lambda_n a_n e_n).\]
    Find all \(A\)-submodules of \(V\).
\end{prob}


\begin{prob}[S2006-Q1]
    Let \(\mathbb{F}_p\) be the field with \(p\) elements, here \(p\) is prime. Let \(\text{SL}_2(\mathbb{F}_p)\) be the group of \(2 \times 2\) matrices over \(\mathbb{F}_p\) with determinant 1.
    \begin{itemize}
        \item[(1)] Find the order of \(\text{SL}_2(\mathbb{F}_p)\). Deduce that
        \[H = \left\{ \begin{pmatrix} 1 & a \\ 0 & 1 \end{pmatrix} \mid a \in \mathbb{F}_p \right\}\]
        is a Sylow-subgroup of \(\text{SL}_2(\mathbb{F}_p)\).
        \item[(2)] Determine the normalizer of \(H\) in \(\text{SL}_2(\mathbb{F}_p)\) and find its order.
    \end{itemize}
\end{prob}


\begin{prob}[S2004-Q1]
    Let \(\mathbb{F}_2\) be the finite field with 2 elements.
    \begin{itemize}
        \item[(a)] What is the order of \(\text{GL}_3(\mathbb{F}_2)\), the group of \(3 \times 3\) invertible matrices over \(\mathbb{F}_2\)?
        \item[(b)] Assuming the fact that \(\text{GL}_3(\mathbb{F}_2)\) is a simple group, find the number of elements of order 7 in \(\text{GL}_3(\mathbb{F}_2)\).
    \end{itemize}
\end{prob}

\begin{prob}[S2002-Q4]
    For a field \(K\), let \(\text{SL}_2(K)\) be the special linear group over \(K\), i.e. the group of \(2 \times 2\)-matrices over \(K\) with determinant 1, and let \(\text{PSL}_2(K)\) be the quotient of \(\text{SL}_2(K)\) by its center, i.e. the projective special linear group. Find the order of \(\text{PSL}_2(F_7)\) where \(F_7\) denotes the finite field of 7 elements.
\end{prob}


\begin{prob}[S2007-Q4]
    Find the invertible elements, the zero divisors and the nilpotent elements in the following rings:
    \begin{itemize}
        \item[(a)] \(\mathbb{Z}/p^n\mathbb{Z}\), where \(n\) is a natural number, \(p\) is a prime one.
        \item[(b)] the upper triangular matrices over a field.
    \end{itemize}
\end{prob}

